---
title: "Probability of Sample Statistics"
author: "ENGEN102-23B (HAM) & (SEC) - Engineering Maths and Modelling 1B"
format:
  revealjs: 
    slide-number: c/t
    progress: true
    auto-stretch: false
    height: 1080
    width: 1920
    theme: reveal-extra/quartz.scss
    template-partials:
      - reveal-extra/title-block.html
    highlight-style: ayu
    code-line-numbers: false
    embed-resources: true
editor: source
---


## Assigning probabilities to events

```{r setup}
#| include: false
library(ggplot2)
```

::::: {.columns}
:::: {.column width="55%" .margin-bottom}
A **probability** is a continuous numeric value that takes a value between 0 and 1 (inclusive)

::: {.fragment}
An **event** is a set of *at least* one distinct outcome
:::

::: {.fragment}
We *commonly* assign a **probability** to an **event** based on the ["long-run"]{.fragment .highlight-red} frequency of the event
:::
::::

:::: {.column width="45%"}
![](https://freepngimg.com/thumb/fruit/175446-basket-vector-fruits-free-download-image.png){style="display: block; margin: auto;"}

::: {style="color: #26351c; font-size: 65%; text-align: center;"}
**Credit:** [Alexis Bailey](https://freepngimg.com/png/175446-basket-vector-fruits-free-download-image), CC BY-NC 4.0
:::
::::
:::::

## An event for continuous numeric values {.margin-bottom style="width: 90%;"}

Suppose you have an *automated* circular saw to cut timber with a 4 metre length at the 2 metre mark

. . .

The specification limits for the automated circular saw to cut the timber is between 1.95 and 2.05 metres, i.e. 2 &pm; 0.05 metres

. . .

How would you define an event for the above?

. . .

Hence we define an event as an interval for continuous numeric values

## Random variable {.margin-bottom style="width: 90%;"}

A **random variable** is denoted with capital letters: $X$, $Y$, etc., and it is a *function* that assigns to every outcome a *non-negative* real number that follows the probability axioms^[See this [Wikipedia](https://en.wikipedia.org/wiki/Probability_axioms) page if you are interested in what these axioms are]

. . .

The values a **random variable** could take are denoted with lowercase letters: $x$, $y$, etc.

. . .

Hence a **random variable** *models* the outcomes we could potentially observe from a random process

## Probability distribution {.margin-bottom}

The *functions* we use to map every outcome to a *non-negative* real number are known as **probability distributions**

. . .

Some well-known **probability distributions** include:

::: {.tightList}
-   The Normal distribution
-   The Binomial distribution
-   The Poisson distribution
:::

## Normal distribution^[Also known as the Gaussian distribution]

:::: {style="margin-bottom: 25px;"}
Let $X$ be a random variable that follows the Normal distribution, that is,

::: {.definition style="width: 50%; display: block; margin: auto;"}
$$
X \sim \text{Normal}(\mu, \sigma)
$$
:::
::::

. . . 

The function a Normally distributed random variable uses is

::: {.definition-orchid style="width: 70%; display: block; margin: auto;"}
$$
f_X(x; \mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-(x-\mu)^2/2\sigma^2}, \forall ~ x \in \mathbb{R}
$$
:::

where

::: {.incremental .tightList style="font-size: 0.9em; margin-left: 0.5em;"}
- $\mu$ is the underlying mean
- $\sigma$ is the underlying standard deviation
- and $\mu \in \mathbb{R}$ and $\sigma > 0$
:::

## [Ex 3:]{style="color: tomato"} Axles under stress

> A group of engineers investigated the final crack depth of railway axles after 10^6^ loading cycles to quantify the durability of the material used for railway axles. In particular, whether the Normal distribution is a suitable model for describing the observed distribution of final crack depths.

:::::: {.columns}
::::: {.column style="width: 45%;"}
:::: {style="font-size: 0.75em;"}
| [Variables]{style="color: #4758AB;"} | |
| :-           | :------------ |
| **axle.id** | A numeric variable which denotes the railway axle's identification number |
| **depth** | A numeric variable which denotes the final crack depth of the railway axle (millimetres) |
::::
:::::

::::: {.column .fragment style="width: 55%;"}
:::: {style="width: 60%; display: block; margin: auto;"}
```{r}
#| eval: false
#| results: asis
#| classes: excel
depth.df <- read.csv("datasets/final-crack-depth.csv")

rbind(colnames(depth.df), head(depth.df, n = 7), rep("...", ncol(depth.df))) |>
# rbind(colnames(davidsSample.df), davidsSample.df) |>
  knitr::kable(format = "html", row.names = TRUE, col.names = LETTERS[1:ncol(depth.df)])
```
::: {.excel-caption}
This is the `final-crack-depth.csv` data from [Moodle]{.Moodle}
:::
::::
:::::
::::::

::: {.aside}
Based on Figure 10 from Yasniy, O., Lapusta, Y., Pyndus, Y., Sorochak, A., & Yasniy, V. (2013). Assessment of lifetime of railway axle. *International Journal of Fatigue* **50**, 40--46.
:::

## Fitting the Normal distribution to data

:::::: {.columns}
::::: {.column style="width: 55%" .margin-bottom}
A numeric variable with a symmetric and bell-shaped distribution tends to be modelled well by a Normal distribution

:::: {.fragment}
If you **assume** that your data is $X \sim \text{Normal}(\mu, \sigma)$:

::: {.tightList .incremental style="font-size: 0.8em; margin-left: 0.25em;"}
- The sample mean, $\bar{x}$, is the best estimate of the underlying mean $\mu$
- The sample standard deviation, $s$, is the best estimate of the underlying standard deviation $\sigma$
:::

::::

:::: {.fragment}
[iNZight]{.R} calculates that $\bar{x} = 5.49 ~ (2 ~\text{dp})$ and $s = 0.06 ~ (2 ~\text{dp})$ for [Ex 3]{style="color: tomato;"}
::::

:::::
::::: {.column style="width: 45%;"}
:::: {.r-stack}
::: {.fragment .fade-out fragment-index=1}
```{r}
#| eval: false
#| fig-dpi: 300
#| fig-height: 7
#| fig-width: 7
plotObj <- ggplot(data = depth.df, aes(x = depth, y = after_stat(density))) +
  geom_histogram(fill = "lightblue", colour = "black", bins = 7) +
  labs(x = "Final Crack Depth (mm)", title = "Distribution of Railway Axles' Final Crack Depths", y = "Density") +
  theme_bw() +
  geom_function(inherit.aes = FALSE, fun = dnorm, args = list(mean = mean(depth.df$depth), sd = sd(depth.df$depth)), xlim = mean(depth.df$depth) + 4 * c(-1, 1) *  sd(depth.df$depth), colour = NA, linewidth = 1.25)
plotObj
```
:::

::: {.fragment fragment-index=1}
```{r}
#| eval: false
#| fig-dpi: 300
#| fig-height: 7
#| fig-width: 7
plotObj +
  geom_function(inherit.aes = FALSE, fun = dnorm, args = list(mean = mean(depth.df$depth), sd = sd(depth.df$depth)), xlim = mean(depth.df$depth) + 4 * c(-1, 1) *  sd(depth.df$depth), colour = "black", linewidth = 1.25)

```
:::
::::
:::::
::::::

## Events with the Normal distribution {.margin-bottom}

Since a random variable that is Normally distributed assigns $f_X(x) ~ \forall ~ x$, we call it a **continuous** random variable. So for any $X \sim \text{Normal}(\mu, \sigma)$:

:::::: {.columns}
::::: {.column .tightList}
<ol>
  <li class="fragment" data-fragment-index=1>$\mathbb{P}(a < X < b) > 0$</li>
  <li class="fragment" data-fragment-index=2>$\mathbb{P}(X = x) = 0$</li>
  <li class="fragment" data-fragment-index=3>$\mathbb{P}(-|b| < X < -|a|) = \mathbb{P} (a < X < b)$</li>
  <li class="fragment" data-fragment-index=4>$\mathbb{P}(X < x) = 1 - \mathbb{P}(X > x)$</li>
  <li class="fragment" data-fragment-index=5>
    $\mathbb{P}(a < X < b \cup c < X < d) =$
    
    $~~\mathbb{P}(a < X < b) + \mathbb{P}(c < X < d)$
  </li>
</ol>
:::::
::::: {.column}
:::: {.r-stack}
::: {.fragment .fade-out fragment-index=1}
```{r}
#| eval: false
#| fig-dpi: 300
#| out-width: 90%
#| fig-align: center
#| fig-width: 7
#| fig-height: 5

par(mai = c(0.6, 0.65, 0, 0))
# z <- seq(0, 1.5, length.out = 1000)
z0 <- seq(-4, 4, length.out = 1000)
plot(0, type = "n", xlim = c(-3, 3), ylim = c(0, dnorm(0) * 1.1), axes = FALSE, yaxs = "i", xlab = "", ylab = "")
lines(x = z0, y = dnorm(z0), col = "gray")
abline(h = 0)
axis(1, tcl = -0.2, mgp = c(0.75, 0.5, 0)); mtext("x", side = 1, line = 1.7, font = 2)
axis(2, tcl = -0.2, mgp = c(0.75, 0.5, 0), las = 2); mtext("f(x; μ, σ)", side = 2, line = 2.25, font = 2)

# zz <- seq(-1.5, -0.5, length.out = 1000)

# polygon(x = c(zz, rev(zz)), y = c(dnorm(zz), rep(0, length(zz))), col = "yellow", lwd = 2)
```
:::
::: {.fragment .fade-in-then-out fragment-index=1}
```{r}
#| eval: false
#| fig-dpi: 300
#| out-width: 90%
#| fig-align: center
#| fig-width: 7
#| fig-height: 5

par(mai = c(0.6, 0.65, 0, 0))
# z <- seq(0, 1.5, length.out = 1000)
z0 <- seq(-4, 4, length.out = 1000)
plot(0, type = "n", xlim = c(-3, 3), ylim = c(0, dnorm(0) * 1.1), axes = FALSE, yaxs = "i", xlab = "", ylab = "")
lines(x = z0, y = dnorm(z0), col = "gray")
abline(h = 0)
axis(1, tcl = -0.2, mgp = c(0.75, 0.5, 0)); mtext("x", side = 1, line = 1.7, font = 2)
axis(2, tcl = -0.2, mgp = c(0.75, 0.5, 0), las = 2); mtext("f(x; μ, σ)", side = 2, line = 2.25, font = 2)

zz <- seq(1, 2, length.out = 1000)

polygon(x = c(zz, rev(zz)), y = c(dnorm(zz), rep(0, length(zz))), col = "yellow", lwd = 2)
```
:::
::: {.fragment .fade-in-then-out fragment-index=2}
```{r}
#| eval: false
#| fig-dpi: 300
#| out-width: 90%
#| fig-align: center
#| fig-width: 7
#| fig-height: 5

par(mai = c(0.6, 0.65, 0, 0))
# z <- seq(0, 1.5, length.out = 1000)
z0 <- seq(-4, 4, length.out = 1000)
plot(0, type = "n", xlim = c(-3, 3), ylim = c(0, dnorm(0) * 1.1), axes = FALSE, yaxs = "i", xlab = "", ylab = "")
lines(x = z0, y = dnorm(z0), col = "gray")
abline(h = 0)
axis(1, tcl = -0.2, mgp = c(0.75, 0.5, 0)); mtext("x", side = 1, line = 1.7, font = 2)
axis(2, tcl = -0.2, mgp = c(0.75, 0.5, 0), las = 2); mtext("f(x; μ, σ)", side = 2, line = 2.25, font = 2)

zz <- seq(2, 2, length.out = 1000)

polygon(x = c(zz, rev(zz)), y = c(dnorm(zz), rep(0, length(zz))), col = "yellow", lwd = 2)
```
:::
::: {.fragment .fade-in-then-out fragment-index=3}
```{r}
#| eval: false
#| fig-dpi: 300
#| out-width: 90%
#| fig-align: center
#| fig-width: 7
#| fig-height: 5

par(mai = c(0.6, 0.65, 0, 0))
# z <- seq(0, 1.5, length.out = 1000)
z0 <- seq(-4, 4, length.out = 1000)
plot(0, type = "n", xlim = c(-3, 3), ylim = c(0, dnorm(0) * 1.1), axes = FALSE, yaxs = "i", xlab = "", ylab = "")
lines(x = z0, y = dnorm(z0), col = "gray")
abline(h = 0)
axis(1, tcl = -0.2, mgp = c(0.75, 0.5, 0)); mtext("x", side = 1, line = 1.7, font = 2)
axis(2, tcl = -0.2, mgp = c(0.75, 0.5, 0), las = 2); mtext("f(x; μ, σ)", side = 2, line = 2.25, font = 2)

zz <- seq(1, 2, length.out = 1000)

polygon(x = c(zz, rev(zz)), y = c(dnorm(zz), rep(0, length(zz))), col = "lemonchiffon", lwd = 2)

zz <- seq(-2, -1, length.out = 1000)

polygon(x = c(zz, rev(zz)), y = c(dnorm(zz), rep(0, length(zz))), col = "yellow", lwd = 2)
```
:::
::: {.fragment .fade-in-then-out fragment-index=4}
```{r}
#| eval: false
#| fig-dpi: 300
#| out-width: 90%
#| fig-align: center
#| fig-width: 7
#| fig-height: 5

par(mai = c(0.6, 0.65, 0, 0))
# z <- seq(0, 1.5, length.out = 1000)
z0 <- seq(-4, 4, length.out = 1000)
plot(0, type = "n", xlim = c(-3, 3), ylim = c(0, dnorm(0) * 1.1), axes = FALSE, yaxs = "i", xlab = "", ylab = "")
lines(x = z0, y = dnorm(z0), col = "gray")
abline(h = 0)
axis(1, tcl = -0.2, mgp = c(0.75, 0.5, 0)); mtext("x", side = 1, line = 1.7, font = 2)
axis(2, tcl = -0.2, mgp = c(0.75, 0.5, 0), las = 2); mtext("f(x; μ, σ)", side = 2, line = 2.25, font = 2)

zz <- seq(-4, 1, length.out = 1000)

polygon(x = c(zz, rev(zz)), y = c(dnorm(zz), rep(0, length(zz))), col = "yellow", lwd = 2)


zz <- seq(1, 4, length.out = 1000)

polygon(x = c(zz, rev(zz)), y = c(dnorm(zz), rep(0, length(zz))), col = "lemonchiffon", lwd = 2)
```
:::
::: {.fragment .fade-in-then-out fragment-index=5}
```{r}
#| eval: false
#| fig-dpi: 300
#| out-width: 90%
#| fig-align: center
#| fig-width: 7
#| fig-height: 5

par(mai = c(0.6, 0.65, 0, 0))
# z <- seq(0, 1.5, length.out = 1000)
z0 <- seq(-4, 4, length.out = 1000)
plot(0, type = "n", xlim = c(-3, 3), ylim = c(0, dnorm(0) * 1.1), axes = FALSE, yaxs = "i", xlab = "", ylab = "")
lines(x = z0, y = dnorm(z0), col = "gray")
abline(h = 0)
axis(1, tcl = -0.2, mgp = c(0.75, 0.5, 0)); mtext("x", side = 1, line = 1.7, font = 2)
axis(2, tcl = -0.2, mgp = c(0.75, 0.5, 0), las = 2); mtext("f(x; μ, σ)", side = 2, line = 2.25, font = 2)

zz <- seq(-3, -2, length.out = 1000)

polygon(x = c(zz, rev(zz)), y = c(dnorm(zz), rep(0, length(zz))), col = "yellow", lwd = 2)

zz <- seq(1, 2, length.out = 1000)

polygon(x = c(zz, rev(zz)), y = c(dnorm(zz), rep(0, length(zz))), col = "yellow", lwd = 2)
```
:::
::::
:::::
::::::

## Calculating probabilities by hand {.margin-bottom}

For **continuous** random variables, we integrate the function specified by the **probability distribution**, that is,

. . .

::: {.definition style="width: 55%; display: block; margin: auto;"}
$$
\mathbb{P}(a < X < b) = \int^b_a f_X(x;\theta)\, dx
$$
:::

. . .

::: {style="margin-top: 25px;"}
However, integrating the function specified by a Normal distribution is not trivial^[Sean is better equipped to answer this question]
:::

. . .

So calculating probabilities for $X \sim \text{Normal}(\mu, \sigma)$ by hand involves **Statistical Tables** that are available on [Moodle]{.Moodle}

## Change of variable from *X* to *Z* {.margin-bottom}

**Table A** presents $\mathbb{P}(0 < Z < z)$ to four decimal places, where $Z \sim \text{Normal}(\mu = 0, \sigma = 1)$^[A Normal distribution with these parameters is more commonly known as the Standard Normal distribution]

. . .

To use **Table A** for any $X \sim \text{Normal}(\mu, \sigma)$, we standardise the bounds of the interval in terms of *z*-scores

. . .

::: {.definition style="width: 30%; display: block; margin: auto;"}
$$
z = \frac{x - \mu}{\sigma}
$$
:::

. . .

::: {style="margin-top: 25px;"}
For example, if $\mathbb{P}(a < X < b)$, the equivalent probability statement in terms of $Z \sim \text{Normal}(\mu = 0, \sigma = 1)$ is:
:::

[$~~ \mathbb{P}\!\left(\frac{a - \mu}{\sigma} < Z < \frac{b - \mu}{\sigma}\right)$]{.fragment}

## Calculating probabilities by hand [(cont'd)]{style="font-size: 0.5em;"}

::: {.incremental}
1. Let $A \sim \text{Normal}(\mu = 4, \sigma = 2)$. Calculate $\mathbb{P}(A < 3)$
2. Let $B \sim \text{Normal}(\mu = 3, \sigma = 1.5)$. Calculate $\mathbb{P}(1.5 < B < 4.5)$
3. Let $C \sim \text{Normal}(\mu = 7, \sigma = 3.5)$. Calculate $\mathbb{P}(7 < C < 14)$
:::

## Empirical rule for the Normal distribution

For any $X \sim \text{Normal}(\mu, \sigma)$:

:::::: {.columns}
::::: {.column .tightList}
<ul>
  <li class="fragment" data-fragment-index=1>$\mathbb{P}(\mu - \sigma < X < \mu + \sigma) \approx 0.6827$</li>
  <li class="fragment" data-fragment-index=2>$\mathbb{P}(\mu - 2\sigma < X < \mu + 2\sigma) \approx 0.9545$</li>
  <li class="fragment" data-fragment-index=3>$\mathbb{P}(\mu - 3\sigma < X < \mu + 3\sigma) \approx 0.9973$</li>
</ul>
:::::
::::: {.column}
:::: {.r-stack}
::: {.fragment .fade-out fragment-index=1}
```{r}
#| eval: false
#| fig-dpi: 300
#| out-width: 90%
#| fig-align: center
#| fig-width: 7
#| fig-height: 5

par(mai = c(0.6, 0.65, 0, 0))
# z <- seq(0, 1.5, length.out = 1000)
z0 <- seq(-4, 4, length.out = 1000)
plot(0, type = "n", xlim = c(-3, 3), ylim = c(0, dnorm(0) * 1.1), axes = FALSE, yaxs = "i", xlab = "", ylab = "")
lines(x = z0, y = dnorm(z0), col = "gray")
abline(h = 0)
axis(1, tcl = -0.2, mgp = c(0.75, 0.5, 0)); mtext("x", side = 1, line = 1.7, font = 2)
axis(2, tcl = -0.2, mgp = c(0.75, 0.5, 0), las = 2); mtext("f(x; μ, σ)", side = 2, line = 2.25, font = 2)
```
:::
::: {.fragment .fade-in-then-out fragment-index=1}
```{r}
#| echo: false
#| fig-dpi: 300
#| out-width: 90%
#| fig-align: center
#| fig-width: 7
#| fig-height: 5

par(mai = c(0.6, 0.65, 0, 0))
# z <- seq(0, 1.5, length.out = 1000)
z0 <- seq(-4, 4, length.out = 1000)
plot(0, type = "n", xlim = c(-3, 3), ylim = c(0, dnorm(0) * 1.1), axes = FALSE, yaxs = "i", xlab = "", ylab = "")
lines(x = z0, y = dnorm(z0), col = "gray")
abline(h = 0)
axis(1, tcl = -0.2, mgp = c(0.75, 0.5, 0)); mtext("x", side = 1, line = 1.7, font = 2)
axis(2, tcl = -0.2, mgp = c(0.75, 0.5, 0), las = 2); mtext("f(x; μ, σ)", side = 2, line = 2.25, font = 2)

zz <- seq(-1, 1, length.out = 1000)

polygon(x = c(zz, rev(zz)), y = c(dnorm(zz), rep(0, length(zz))), col = "yellow", lwd = 2)
```
:::
::: {.fragment .fade-in-then-out fragment-index=2}
```{r}
#| eval: false
#| fig-dpi: 300
#| out-width: 90%
#| fig-align: center
#| fig-width: 7
#| fig-height: 5

par(mai = c(0.6, 0.65, 0, 0))
# z <- seq(0, 1.5, length.out = 1000)
z0 <- seq(-4, 4, length.out = 1000)
plot(0, type = "n", xlim = c(-3, 3), ylim = c(0, dnorm(0) * 1.1), axes = FALSE, yaxs = "i", xlab = "", ylab = "")
lines(x = z0, y = dnorm(z0), col = "gray")
abline(h = 0)
axis(1, tcl = -0.2, mgp = c(0.75, 0.5, 0)); mtext("x", side = 1, line = 1.7, font = 2)
axis(2, tcl = -0.2, mgp = c(0.75, 0.5, 0), las = 2); mtext("f(x; μ, σ)", side = 2, line = 2.25, font = 2)

zz <- seq(-2, 2, length.out = 1000)

polygon(x = c(zz, rev(zz)), y = c(dnorm(zz), rep(0, length(zz))), col = "yellow", lwd = 2)
```
:::
::: {.fragment .fade-in-then-out fragment-index=3}
```{r}
#| eval: false
#| fig-dpi: 300
#| out-width: 90%
#| fig-align: center
#| fig-width: 7
#| fig-height: 5

par(mai = c(0.6, 0.65, 0, 0))
# z <- seq(0, 1.5, length.out = 1000)
z0 <- seq(-4, 4, length.out = 1000)
plot(0, type = "n", xlim = c(-3, 3), ylim = c(0, dnorm(0) * 1.1), axes = FALSE, yaxs = "i", xlab = "", ylab = "")
lines(x = z0, y = dnorm(z0), col = "gray")
abline(h = 0)
axis(1, tcl = -0.2, mgp = c(0.75, 0.5, 0)); mtext("x", side = 1, line = 1.7, font = 2)
axis(2, tcl = -0.2, mgp = c(0.75, 0.5, 0), las = 2); mtext("f(x; μ, σ)", side = 2, line = 2.25, font = 2)

zz <- seq(-3, 3, length.out = 1000)

polygon(x = c(zz, rev(zz)), y = c(dnorm(zz), rep(0, length(zz))), col = "yellow", lwd = 2)
```
:::
::::
:::::
::::::

## Calculating probabilities with software

::: {style="width: 55%"}
```{python}
#| eval: false
# A Python script to calculate the exact probabilities on Slide 12
from scipy import integrate
from scipy.stats import norm
import math
```
:::

1. Let $A \sim \text{Normal}(\mu = 4, \sigma = 2)$. Calculate $\mathbb{P}(A < 3)$

::: {style="width: 55%"}
```{python}
#| eval: false
integrate.quad(norm.pdf, -math.inf, 3, args=(4, 2))
```
:::

2. Let $B \sim \text{Normal}(\mu = 3, \sigma = 1.5)$. Calculate $\mathbb{P}(1.5 < B < 4.5)$

::: {style="width: 55%"}
```{python}
#| eval: false
integrate.quad(norm.pdf, 1.5, 4.5, args=(3, 1.5))
```
:::

3. Let $C \sim \text{Normal}(\mu = 7, \sigma = 3.5)$. Calculate $\mathbb{P}(7 < C < 14)$

::: {style="width: 55%"}
```{python}
#| eval: false
integrate.quad(norm.pdf, 7, 14, args=(7, 3.5))
```
:::



## Properties of a random variable {.margin-bottom}

Previously, we developed the idea of a **random variable** to model the outcomes we could potentially observe from a random process

. . .

A **random variable** can also be summarised in terms of a mean^[Technically, the **random variable's** expected value], $\mu$, and standard deviation, $\sigma$

. . .

However, a **random variable** effectively represents only a single observation from a random process[. So how could we model more than one observation?]{.fragment}

## What is a sample? {.margin-bottom}

Let's consider a sequence of $n$ **random variables:** $X_1, X_2, \cdots, X_n$

. . .

A reasonable assumption is that all $n$ $X_i$s model the outcomes of the *same* random process[. So we state that $X_1, X_2, \cdots, X_n$ are [identically]{style="color: red;"} distributed]{.fragment}

. . .

A mathematically convenient assumption is that all $n$ $X_i$s are [independent]{style="color: red;"}[. This implies that the outcomes we see for each $X_i$ does not **depend** on other $X_i$...]{.fragment}

. . .

We could develop notation etc. to handle multiple $X_i$s to model the outcomes we could potentially observe in a sample...

## [Sample statistics]{.fragment} {.margin-bottom}

More often than not, we are more interested in whether the sample mean, $\bar{x}$, is an unbiased and precise estimate of the underlying mean $\mu$

. . .

More generally, let $\mu$ be the mean of the **random variable** we are using to model the outcomes we could potentially observe from a random process

. . .

However, $\bar{x}$ would change sample-to-sample...

. . .

Could we use a **random variable** to model potential sample means we could observe from a single sample of $n$ observations?

## Visualising sample-to-sample variation

:::::: {.columns}
::::: {.column style="width: 55%;"}
Recall [Ex 1]{style="color: tomato;"} where we used a sample of 10 blocks to estimate the "true" average block weight

:::: {.fragment style="margin-top: 25px;" fragment-index=0}
Let's carry out a *brief* numerical experiment on the distribution of potential sample means as $n \rightarrow N$ [(as [Ex 1]{style="color: tomato;"} has a finite population size of 100)]{style="font-size: 0.5em;"}
::::
:::::
::::: {.column style="width: 45%;"}
:::: {.r-stack}
::: {.fragment .fade-in-then-out fragment-index=1}
```{r}
#| echo: false
#| fig-dpi: 300
#| out-width: 90%
#| fig-align: center
#| fig-width: 7
#| fig-height: 5

set.seed(1)
block.popn <- read.csv("datasets/blocks-popn.csv")
par(mai = c(0.6, 0, 0, 0))

n <- 10
mus <- numeric(5000)

for (i in 1:length(mus)) {
  x <- sample(block.popn$weight, size = n)
  mus[i] <- mean(x)
}

MASS::truehist(mus, xlim = c(0, 70), prob = TRUE, h = 1, axes = FALSE, col = "lightblue")
legend("topright", legend = paste("n =", n), bty = "n", cex = 1.5)
axis(1, tcl = -0.2, mgp = c(0.75, 0.5, 0)); mtext(bquote(bold(bar(x))), side = 1, line = 1.7, font = 2)
```
:::
::: {.fragment .fade-in-then-out fragment-index=2}
```{r}
#| echo: false
#| fig-dpi: 300
#| out-width: 90%
#| fig-align: center
#| fig-width: 7
#| fig-height: 5

set.seed(2)
par(mai = c(0.6, 0, 0, 0))

n <- 25
mus <- numeric(5000)

for (i in 1:length(mus)) {
  x <- sample(block.popn$weight, size = n)
  mus[i] <- mean(x)
}

MASS::truehist(mus, xlim = c(0, 70), prob = TRUE, h = 1, axes = FALSE, col = "lightblue")
legend("topright", legend = paste("n =", n), bty = "n", cex = 1.5)
axis(1, tcl = -0.2, mgp = c(0.75, 0.5, 0)); mtext(bquote(bold(bar(x))), side = 1, line = 1.7, font = 2)
```
:::
::: {.fragment .fade-in-then-out fragment-index=3}
```{r}
#| echo: false
#| fig-dpi: 300
#| out-width: 90%
#| fig-align: center
#| fig-width: 7
#| fig-height: 5

set.seed(3)
par(mai = c(0.6, 0, 0, 0))

n <- 50
mus <- numeric(5000)

for (i in 1:length(mus)) {
  x <- sample(block.popn$weight, size = n)
  mus[i] <- mean(x)
}

MASS::truehist(mus, xlim = c(0, 70), prob = TRUE, h = 1, axes = FALSE, col = "lightblue")
legend("topright", legend = paste("n =", n), bty = "n", cex = 1.5)
axis(1, tcl = -0.2, mgp = c(0.75, 0.5, 0)); mtext(bquote(bold(bar(x))), side = 1, line = 1.7, font = 2)
```
:::
::: {.fragment .fade-in-then-out fragment-index=4}
```{r}
#| echo: false
#| fig-dpi: 300
#| out-width: 90%
#| fig-align: center
#| fig-width: 7
#| fig-height: 5

set.seed(4)
par(mai = c(0.6, 0, 0, 0))

n <- 100
mus <- numeric(5000)

for (i in 1:length(mus)) {
  x <- sample(block.popn$weight, size = n)
  mus[i] <- mean(x)
}

MASS::truehist(mus, xlim = c(0, 70), prob = TRUE, h = 1, axes = FALSE, col = "lightblue")
legend("topright", legend = paste("n =", n), bty = "n", cex = 1.5)
axis(1, tcl = -0.2, mgp = c(0.75, 0.5, 0)); mtext(bquote(bold(bar(x))), side = 1, line = 1.7, font = 2)
```
:::
::::
:::::
::::::

## Strong Law of Large Numbers {.margin-bottom}

What we just saw was the **Strong Law of Large Numbers**^[This only applies to sample means] in action. It states that

. . .

::: {.definition style="width: 60%;"}
$$
\mathbb{P} \! \left(\lim_{n\rightarrow \infty}\frac{X_1 + X_2 + \cdots X_n}{n} = \mu\right) = 1
$$
:::

. . .

::: {style="margin-top: 25px;"}
Is there a more general result?
:::

## Central Limit Theorem {.margin-bottom}

Let $X_1, X_2, \cdots, X_n$ be $n$ independent and identically distributed **random variables** with finite $\mu_X$ and $\sigma_X$

. . .

Suppose that $S_n = X_1 + X_2 + \cdots + X_n$

. . .

::: {.definition style="width: 70%;"}
$$
S_n \rightarrow_D \text{Normal} \! \left(\mu = n\mu_X, \sigma = \sqrt{n}\,\sigma_X\right)
$$
:::

::: {style="font-size: 0.5em; margin-top: 25px;"}
$\rightarrow_D$ means converges in distribution
:::

. . .

How large does $n$ has to be to ensure the convergence is good enough to use a Normally distributed **random variable** to model $S_n$? 

. . .

$n \geq 30$ for the majority of **probability distributions** with finite $\mu_X$ and $\sigma_X$

## Sampling distribution of *X&#772;* {.margin-bottom}

Let $X_1, X_2, \cdots, X_n$ be $n$  independent and identically distributed **random variables** with finite $\mu_X$ and $\sigma_X$

. . .

Suppose that $\overline{X} = \frac{X_1 \, + X_2 \, + \, \cdots \, + \, X_n}{n}$, then **Central Limit Theorem** states that

. . .

::: {.definition style="width: 70%;"}
$$
\overline{X} \rightarrow_D \text{Normal} \! \left(\mu = \mu_X, \sigma = \frac{\sigma_X}{\sqrt{n}}\right)
$$
:::

. . . 

::: {style="margin-top: 25px;"}
Similarly, $n \geq 30$ is good enough to use a Normally distributed **random variable** to model $\overline{X}$
:::

. . .

Furthermore, if $X_i \sim \text{Normal}(\mu_X, \sigma_X)$: [$\overline{X} \sim \text{Normal} \! \left(\mu = \mu_X, \sigma = \frac{\sigma_X}{\sqrt{n}}\right)$]{.fragment}


## [Ex 4:]{style='color: tomato;'} Beer!

> Suppose a beer is sold in 330 ml bottles. Each bottle is supposed to contain 330 ml, but due to variation in the manufacturing process, the real amount is Normally Distributed with mean 328 ml and standard deviation of 3 ml.

::: {.incremental}
1. Calculate the probability that a randomly selected bottle contains less than 330 ml of beer
2. Calculate the probability that the average volume of beer in a randomly selected crate of 24 bottles is greater than 330 ml
:::

## Calculating [Ex 4]{style='color: tomato;'} with software

::: {style="width: 55%"}
```{python}
#| echo: true
# A Python script to calculate the exact probabilities on Slide 12
from scipy import integrate
from scipy.stats import norm
import math
```
:::

1. Calculate the probability that a randomly selected bottle contains less than 330 ml of beer

::: {style="width: 55%"}
```{python}
#| echo: true
integrate.quad(norm.pdf, -math.inf, 330, args=(328, 3))
```
:::

2. Calculate the probability that the average volume of beer in a randomly selected crate of 24 bottles is greater than 330 ml

::: {style="width: 55%"}
```{python}
#| echo: true
integrate.quad(norm.pdf, 330, math.inf, args=(328, 3/math.sqrt(24)))
```
:::

## Why check assumptions? [--- Bathtub distribution]{.fragment fragment-index=0}

**Central Limit Theorem** and its primary application to the sampling distribution of $\overline{X}$ means that it works with "most" probability distributions once $n$ is large enough

:::::: {.columns}
::::: {.column .fragment fragment-index=0}
```{r}
#| echo: false
#| fig-dpi: 300
#| out-width: 90%
#| fig-align: center
#| fig-width: 7
#| fig-height: 5

set.seed(10)
par(mai = c(0.6, 0, 0, 0))

x <- rbeta(100000, 0.5, 0.5)
# x <- x[which(x < 1)]

MASS::truehist(x, prob = TRUE, axes = FALSE, h = 0.05, x0 = -0.05, col = "tomato")
axis(1, tcl = -0.2, mgp = c(0.75, 0.5, 0)); mtext("x", side = 1, line = 1.7, font = 2)
```
:::::
::::: {.column}
:::: {.r-stack}
::: {.fragment .fade-in-then-out fragment-index=1}
```{r}
#| echo: false
#| fig-dpi: 300
#| out-width: 90%
#| fig-align: center
#| fig-width: 7
#| fig-height: 5

set.seed(11)
par(mai = c(0.6, 0, 0, 0))

n <- 3
mus <- numeric(5000)

for (i in 1:length(mus)) {
  x <- rbeta(n, 0.5, 0.5)
  mus[i] <- mean(x)
}

MASS::truehist(mus, prob = TRUE, h = 0.05, x0 = -0.05, axes = FALSE, col = "lightblue", ylim = c(0, dnorm(0.5, 0.5, sqrt(0.125 / n))), xlim = c(0, 1))
curve(dnorm(x, 0.5, sqrt(0.125 / n)), add = TRUE, lwd = 2)
legend("topright", legend = paste("n =", n), bty = "n", cex = 1.5)
axis(1, tcl = -0.2, mgp = c(0.75, 0.5, 0)); mtext(bquote(bold(bar(x))), side = 1, line = 1.7, font = 2)
```
:::
::: {.fragment .fade-in-then-out fragment-index=2}
```{r}
#| echo: false
#| fig-dpi: 300
#| out-width: 90%
#| fig-align: center
#| fig-width: 7
#| fig-height: 5

set.seed(12)
par(mai = c(0.6, 0, 0, 0))

n <- 15
mus <- numeric(5000)

for (i in 1:length(mus)) {
  x <- rbeta(n, 0.5, 0.5)
  mus[i] <- mean(x)
}

MASS::truehist(mus, prob = TRUE, h = 0.05, x0 = -0.05, axes = FALSE, col = "lightblue", ylim = c(0, dnorm(0.5, 0.5, sqrt(0.125 / n))), xlim = c(0, 1))
curve(dnorm(x, 0.5, sqrt(0.125 / n)), add = TRUE, lwd = 2)
legend("topright", legend = paste("n =", n), bty = "n", cex = 1.5)
axis(1, tcl = -0.2, mgp = c(0.75, 0.5, 0)); mtext(bquote(bold(bar(x))), side = 1, line = 1.7, font = 2)
```
:::
::: {.fragment .fade-in-then-out fragment-index=3}
```{r}
#| echo: false
#| fig-dpi: 300
#| out-width: 90%
#| fig-align: center
#| fig-width: 7
#| fig-height: 5

set.seed(13)
par(mai = c(0.6, 0, 0, 0))

n <- 30
mus <- numeric(5000)

for (i in 1:length(mus)) {
  x <- rbeta(n, 0.5, 0.5)
  mus[i] <- mean(x)
}

MASS::truehist(mus, prob = TRUE, h = 0.05, x0 = -0.05, axes = FALSE, col = "lightblue", ylim = c(0, dnorm(0.5, 0.5, sqrt(0.125 / n))), xlim = c(0, 1))
curve(dnorm(x, 0.5, sqrt(0.125 / n)), add = TRUE, lwd = 2)
legend("topright", legend = paste("n =", n), bty = "n", cex = 1.5)
axis(1, tcl = -0.2, mgp = c(0.75, 0.5, 0)); mtext(bquote(bold(bar(x))), side = 1, line = 1.7, font = 2)
```
:::
::::
:::::
::::::

## Why check assumptions? --- The importance of i.i.d.

The independence assumption is quite peculiar[, but its purpose is to prompt the modeller to consider whether the data was collected properly]{.fragment fragment-index=0}

:::::: {.columns}
::::: {.column}
```{r}
#| echo: false
#| fig-dpi: 300
#| out-width: 90%
#| fig-align: center
#| fig-width: 7
#| fig-height: 5

set.seed(21)
par(mai = c(0.6, 0, 0, 0))

X <- c(rnorm(5000, 0, 5), rnorm(5000, 50, 5))
# x <- x[which(x < 1)]

MASS::truehist(X, prob = TRUE, axes = FALSE, col = "lemonchiffon", h = 2.5, xlim = c(-20, 70))
axis(1, tcl = -0.2, mgp = c(0.75, 0.5, 0)); mtext("x", side = 1, line = 1.7, font = 2)
```
:::::
::::: {.column}
:::: {.r-stack}
::: {.fragment .fade-in-then-out fragment-index=1}
```{r}
#| echo: false
#| fig-dpi: 300
#| out-width: 90%
#| fig-align: center
#| fig-width: 7
#| fig-height: 5

set.seed(22)
par(mai = c(0.6, 0, 0, 0))

n <- 3
mus <- numeric(5000)

for (i in 1:length(mus)) {
  coin <- sample(c(0, 50), size = 1)
  x <- rnorm(n, coin, 5)
  mus[i] <- mean(x)
}

MASS::truehist(mus, prob = TRUE, axes = FALSE, col = "lightblue", xlim = c(-20, 70), h = 2.5)
# curve(dnorm(x, 0.5, sqrt(0.125 / n)), add = TRUE, lwd = 2)
legend("topright", legend = paste("n =", n), bty = "n", cex = 1.5)
axis(1, tcl = -0.2, mgp = c(0.75, 0.5, 0)); mtext(bquote(bold(bar(x))), side = 1, line = 1.7, font = 2)
```
:::
::: {.fragment .fade-in-then-out fragment-index=2}
```{r}
#| echo: false
#| fig-dpi: 300
#| out-width: 90%
#| fig-align: center
#| fig-width: 7
#| fig-height: 5

set.seed(23)
par(mai = c(0.6, 0, 0, 0))

n <- 15
mus <- numeric(5000)

for (i in 1:length(mus)) {
  coin <- sample(c(0, 50), size = 1)
  x <- rnorm(n, coin, 5)
  mus[i] <- mean(x)
}

MASS::truehist(mus, prob = TRUE, axes = FALSE, col = "lightblue", xlim = c(-20, 70), h = 2.5)
# curve(dnorm(x, 0.5, sqrt(0.125 / n)), add = TRUE, lwd = 2)
legend("topright", legend = paste("n =", n), bty = "n", cex = 1.5)
axis(1, tcl = -0.2, mgp = c(0.75, 0.5, 0)); mtext(bquote(bold(bar(x))), side = 1, line = 1.7, font = 2)
```
:::
<!-- ::: {.fragment .fade-in-then-out fragment-index=3} -->
::: {.fragment fragment-index=3}
```{r}
#| echo: false
#| fig-dpi: 300
#| out-width: 90%
#| fig-align: center
#| fig-width: 7
#| fig-height: 5

set.seed(24)
par(mai = c(0.6, 0, 0, 0))

n <- 30
mus <- numeric(5000)

for (i in 1:length(mus)) {
  coin <- sample(c(0, 50), size = 1)
  x <- rnorm(n, coin, 5)
  mus[i] <- mean(x)
}

MASS::truehist(mus, prob = TRUE, axes = FALSE, col = "lightblue", xlim = c(-20, 70), h = 2.5)
# curve(dnorm(x, 0.5, sqrt(0.125 / n)), add = TRUE, lwd = 2)
legend("topright", legend = paste("n =", n), bty = "n", cex = 1.5)
axis(1, tcl = -0.2, mgp = c(0.75, 0.5, 0)); mtext(bquote(bold(bar(x))), side = 1, line = 1.7, font = 2)
```
:::
::::
:::::
::::::
