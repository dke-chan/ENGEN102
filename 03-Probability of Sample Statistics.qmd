---
title: "Probability of Sample Statistics"
author: "ENGEN102-23B (HAM) & (SEC) - Engineering Maths and Modelling 1B"
format:
  revealjs: 
    slide-number: c/t
    progress: true
    auto-stretch: false
    height: 1080
    width: 1920
    theme: reveal-extra/quartz.scss
    template-partials:
      - reveal-extra/title-block.html
    highlight-style: ayu
    code-line-numbers: false
    embed-resources: true
editor: source
---

## Properties of a random variable {.margin-bottom}

Previously, we developed the idea of a **random variable** to model the outcomes we could potentially observe from a random process

. . .

A **random variable** can also be summarised in terms of a mean^[Technically, the **random variable's** expected value], $\mu$, and standard deviation, $\sigma$

. . .

However, a **random variable** effectively represents only a single observation from a random process[. So how could we model more than one observation?]{.fragment}

## Central Limit Theorem {.margin-bottom}

Let $X_1, X_2, \cdots, X_n$ be $n$ independent and identically distributed **random variables** with finite $\mu_X$ and $\sigma_X$

. . .

Suppose that $S_n = X_1 + X_2 + \cdots + X_n$

. . .

::: {.definition style="width: 70%;"}
$$
S_n \rightarrow_D \text{Normal} \! \left(\mu = n\mu_X, \sigma = \sqrt{n}\,\sigma_X\right)
$$
:::

::: {style="font-size: 0.5em; margin-top: 25px;"}
$\rightarrow_D$ means converges in distribution
:::

. . .

How large does $n$ has to be to ensure the convergence is good enough to use a Normally distributed **random variable** to model $S_n$? 

. . .

$n \geq 30$ for the majority of **probability distributions** with finite $\mu_X$ and $\sigma_X$

## Sampling distribution of *X&#772;* {.margin-bottom}

Let $X_1, X_2, \cdots, X_n$ be $n$  independent and identically distributed **random variables** with finite $\mu_X$ and $\sigma_X$

. . .

Suppose that $\overline{X} = \frac{X_1 \, + X_2 \, + \, \cdots \, + \, X_n}{n}$, then **Central Limit Theorem** states that

. . .

::: {.definition style="width: 70%;"}
$$
\overline{X} \rightarrow_D \text{Normal} \! \left(\mu = \mu_X, \sigma = \frac{\sigma_X}{\sqrt{n}}\right)
$$
:::

. . . 

::: {style="margin-top: 25px;"}
Similarly, $n \geq 30$ is good enough to use a Normally distributed **random variable** to model $\overline{X}$
:::

. . .

Furthermore, if $X_i \sim \text{Normal}(\mu_X, \sigma_X)$: [$\overline{X} \sim \text{Normal} \! \left(\mu = \mu_X, \sigma = \frac{\sigma_X}{\sqrt{n}}\right)$]{.fragment}


## [Ex 4:]{style='color: tomato;'} Beer!

> Suppose a beer is sold in 330 ml bottles. Each bottle is supposed to contain 330 ml, but due to variation in the manufacturing process, the real amount is Normally Distributed with mean 328 ml and standard deviation of 3 ml.

::: {.incremental}
1. Calculate the probability that a randomly selected bottle contains less than 330 ml of beer
2. Calculate the probability that the average volume of beer in a randomly selected crate of 24 bottles is greater than 330 ml
:::

## Calculating [Ex 4]{style='color: tomato;'} with software

::: {style="width: 55%"}
```{python}
#| echo: true
# A Python script to calculate the exact probabilities on Slide 12
from scipy import integrate
from scipy.stats import norm
import math
```
:::

1. Calculate the probability that a randomly selected bottle contains less than 330 ml of beer

::: {style="width: 55%"}
```{python}
#| echo: true
integrate.quad(norm.pdf, -math.inf, 330, args=(328, 3))
```
:::

2. Calculate the probability that the average volume of beer in a randomly selected crate of 24 bottles is greater than 330 ml

::: {style="width: 55%"}
```{python}
#| echo: true
integrate.quad(norm.pdf, 330, math.inf, args=(328, 3/math.sqrt(24)))
```
:::

## Why check assumptions? [--- Bathtub distribution]{.fragment fragment-index=0}

**Central Limit Theorem** and its primary application to the sampling distribution of $\overline{X}$ means that it works with "most" probability distributions once $n$ is large enough

:::::: {.columns}
::::: {.column .fragment fragment-index=0}
```{r}
#| echo: false
#| fig-dpi: 300
#| out-width: 90%
#| fig-align: center
#| fig-width: 7
#| fig-height: 5

set.seed(10)
par(mai = c(0.6, 0, 0, 0))

x <- rbeta(100000, 0.5, 0.5)
# x <- x[which(x < 1)]

MASS::truehist(x, prob = TRUE, axes = FALSE, h = 0.05, x0 = -0.05, col = "tomato")
axis(1, tcl = -0.2, mgp = c(0.75, 0.5, 0)); mtext("x", side = 1, line = 1.7, font = 2)
```
:::::
::::: {.column}
:::: {.r-stack}
::: {.fragment .fade-in-then-out fragment-index=1}
```{r}
#| echo: false
#| fig-dpi: 300
#| out-width: 90%
#| fig-align: center
#| fig-width: 7
#| fig-height: 5

set.seed(11)
par(mai = c(0.6, 0, 0, 0))

n <- 3
mus <- numeric(5000)

for (i in 1:length(mus)) {
  x <- rbeta(n, 0.5, 0.5)
  mus[i] <- mean(x)
}

MASS::truehist(mus, prob = TRUE, h = 0.05, x0 = -0.05, axes = FALSE, col = "lightblue", ylim = c(0, dnorm(0.5, 0.5, sqrt(0.125 / n))), xlim = c(0, 1))
curve(dnorm(x, 0.5, sqrt(0.125 / n)), add = TRUE, lwd = 2)
legend("topright", legend = paste("n =", n), bty = "n", cex = 1.5)
axis(1, tcl = -0.2, mgp = c(0.75, 0.5, 0)); mtext(bquote(bold(bar(x))), side = 1, line = 1.7, font = 2)
```
:::
::: {.fragment .fade-in-then-out fragment-index=2}
```{r}
#| echo: false
#| fig-dpi: 300
#| out-width: 90%
#| fig-align: center
#| fig-width: 7
#| fig-height: 5

set.seed(12)
par(mai = c(0.6, 0, 0, 0))

n <- 15
mus <- numeric(5000)

for (i in 1:length(mus)) {
  x <- rbeta(n, 0.5, 0.5)
  mus[i] <- mean(x)
}

MASS::truehist(mus, prob = TRUE, h = 0.05, x0 = -0.05, axes = FALSE, col = "lightblue", ylim = c(0, dnorm(0.5, 0.5, sqrt(0.125 / n))), xlim = c(0, 1))
curve(dnorm(x, 0.5, sqrt(0.125 / n)), add = TRUE, lwd = 2)
legend("topright", legend = paste("n =", n), bty = "n", cex = 1.5)
axis(1, tcl = -0.2, mgp = c(0.75, 0.5, 0)); mtext(bquote(bold(bar(x))), side = 1, line = 1.7, font = 2)
```
:::
::: {.fragment .fade-in-then-out fragment-index=3}
```{r}
#| echo: false
#| fig-dpi: 300
#| out-width: 90%
#| fig-align: center
#| fig-width: 7
#| fig-height: 5

set.seed(13)
par(mai = c(0.6, 0, 0, 0))

n <- 30
mus <- numeric(5000)

for (i in 1:length(mus)) {
  x <- rbeta(n, 0.5, 0.5)
  mus[i] <- mean(x)
}

MASS::truehist(mus, prob = TRUE, h = 0.05, x0 = -0.05, axes = FALSE, col = "lightblue", ylim = c(0, dnorm(0.5, 0.5, sqrt(0.125 / n))), xlim = c(0, 1))
curve(dnorm(x, 0.5, sqrt(0.125 / n)), add = TRUE, lwd = 2)
legend("topright", legend = paste("n =", n), bty = "n", cex = 1.5)
axis(1, tcl = -0.2, mgp = c(0.75, 0.5, 0)); mtext(bquote(bold(bar(x))), side = 1, line = 1.7, font = 2)
```
:::
::::
:::::
::::::

## Why check assumptions? --- The importance of i.i.d.

The independence assumption is quite peculiar[, but its purpose is to prompt the modeller to consider whether the data was collected properly]{.fragment fragment-index=0}

:::::: {.columns}
::::: {.column}
```{r}
#| echo: false
#| fig-dpi: 300
#| out-width: 90%
#| fig-align: center
#| fig-width: 7
#| fig-height: 5

set.seed(21)
par(mai = c(0.6, 0, 0, 0))

X <- c(rnorm(5000, 0, 5), rnorm(5000, 50, 5))
# x <- x[which(x < 1)]

MASS::truehist(X, prob = TRUE, axes = FALSE, col = "lemonchiffon", h = 2.5, xlim = c(-20, 70))
axis(1, tcl = -0.2, mgp = c(0.75, 0.5, 0)); mtext("x", side = 1, line = 1.7, font = 2)
```
:::::
::::: {.column}
:::: {.r-stack}
::: {.fragment .fade-in-then-out fragment-index=1}
```{r}
#| echo: false
#| fig-dpi: 300
#| out-width: 90%
#| fig-align: center
#| fig-width: 7
#| fig-height: 5

set.seed(22)
par(mai = c(0.6, 0, 0, 0))

n <- 3
mus <- numeric(5000)

for (i in 1:length(mus)) {
  coin <- sample(c(0, 50), size = 1)
  x <- rnorm(n, coin, 5)
  mus[i] <- mean(x)
}

MASS::truehist(mus, prob = TRUE, axes = FALSE, col = "lightblue", xlim = c(-20, 70), h = 2.5)
# curve(dnorm(x, 0.5, sqrt(0.125 / n)), add = TRUE, lwd = 2)
legend("topright", legend = paste("n =", n), bty = "n", cex = 1.5)
axis(1, tcl = -0.2, mgp = c(0.75, 0.5, 0)); mtext(bquote(bold(bar(x))), side = 1, line = 1.7, font = 2)
```
:::
::: {.fragment .fade-in-then-out fragment-index=2}
```{r}
#| echo: false
#| fig-dpi: 300
#| out-width: 90%
#| fig-align: center
#| fig-width: 7
#| fig-height: 5

set.seed(23)
par(mai = c(0.6, 0, 0, 0))

n <- 15
mus <- numeric(5000)

for (i in 1:length(mus)) {
  coin <- sample(c(0, 50), size = 1)
  x <- rnorm(n, coin, 5)
  mus[i] <- mean(x)
}

MASS::truehist(mus, prob = TRUE, axes = FALSE, col = "lightblue", xlim = c(-20, 70), h = 2.5)
# curve(dnorm(x, 0.5, sqrt(0.125 / n)), add = TRUE, lwd = 2)
legend("topright", legend = paste("n =", n), bty = "n", cex = 1.5)
axis(1, tcl = -0.2, mgp = c(0.75, 0.5, 0)); mtext(bquote(bold(bar(x))), side = 1, line = 1.7, font = 2)
```
:::
<!-- ::: {.fragment .fade-in-then-out fragment-index=3} -->
::: {.fragment fragment-index=3}
```{r}
#| echo: false
#| fig-dpi: 300
#| out-width: 90%
#| fig-align: center
#| fig-width: 7
#| fig-height: 5

set.seed(24)
par(mai = c(0.6, 0, 0, 0))

n <- 30
mus <- numeric(5000)

for (i in 1:length(mus)) {
  coin <- sample(c(0, 50), size = 1)
  x <- rnorm(n, coin, 5)
  mus[i] <- mean(x)
}

MASS::truehist(mus, prob = TRUE, axes = FALSE, col = "lightblue", xlim = c(-20, 70), h = 2.5)
# curve(dnorm(x, 0.5, sqrt(0.125 / n)), add = TRUE, lwd = 2)
legend("topright", legend = paste("n =", n), bty = "n", cex = 1.5)
axis(1, tcl = -0.2, mgp = c(0.75, 0.5, 0)); mtext(bquote(bold(bar(x))), side = 1, line = 1.7, font = 2)
```
:::
::::
:::::
::::::
