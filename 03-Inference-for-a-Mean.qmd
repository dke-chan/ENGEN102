---
title: "Inference for a Mean"
author: "ENGEN102-23G (HAM) - Engineering Maths and Modelling 1B"
format:
  revealjs: 
    slide-number: c/t
    progress: true
    auto-stretch: false
    height: 1080
    width: 1920
    theme: reveal-extra/quartz.scss
    template-partials:
      - reveal-extra/title-block.html
    highlight-style: breeze
    code-line-numbers: false
    embed-resources: false
editor: source
---

```{r setup}
#| include: false
library(knitr)
library(ggplot2)
```

## Motivation

::: {style="margin-bottom: 50px;"}
The kind of data we typically analyse when we measure one numeric variable on each observation. We often collect this data to *infer* the true mean or *test* sample data to see whether they follow regulations.
:::

. . .

> Sometimes engineers work with problems for which there is no simple or well-understood mechanistic model that explains the phenomenon.

::: {style="font-size: 0.9em; text-align: right; margin-top: -0.5em;"}
 --- Montgomery et al. (2010)
:::

::: {.aside}
Montgomery, D. C., Runger, G. C., & Hubele, N. F. (2010). *Engineering Statistics* (5th ed.). Wiley.
:::

## Statistical Inference

> Statistical inference is the process of drawing conclusions about the entire population based on the information in a sample.

::: {style="font-size: 0.9em; text-align: right; margin-top: -0.5em;"}
 --- Lock et al. (2021)
:::

::: {.aside}
Lock, R. H., Lock P. F., Morgan, K. L., Lock, E. F., & Lock, D. F. (2021). *Statistics: Unlocking the power of data* (3rd ed.). Wiley.
:::

## Statistical Inference in Pictures

:::: {.r-stack}
![](figures\StepOne.png){style="display: block; margin-left: auto; margin-right: auto; width: 90%; border: 1px solid #383d3d;"}

::: {.fragment}
![](figures\StepTwo.png){style="display: block; margin-left: auto; margin-right: auto; width: 90%; border: 1px solid #383d3d;"}
:::

::: {.fragment}
![](figures\StepThree.png){style="display: block; margin-left: auto; margin-right: auto; width: 90%; border: 1px solid #383d3d;"}
:::

::: {.fragment}
![](figures\StepFour.png){style="display: block; margin-left: auto; margin-right: auto; width: 90%; border: 1px solid #383d3d;"}
:::

::: {.fragment}
![](figures\StepFive.png){style="display: block; margin-left: auto; margin-right: auto; width: 90%; border: 1px solid #383d3d;"}
:::
::::

::: {.aside} 
person by Brian Dys Sahagun from [Noun Project](https://thenounproject.com/browse/icons/term/person/) (CC BY 3.0).
:::

## Sampling Distribution of the Sample Mean [(revisited)]{style="font-size: 0.7em;"}

One outcome of the *Central Limit Theorem* is the **sampling distribution of the sample mean**.

. . .

::: {.definition style="margin-top: 50px; width: 90%;"}
Let $X_1, X_2, \cdots, X_n$ be $n$ independent and identically distributed **random variables** with *finite* mean, [$\mu_X$]{.blue}, and standard deviation, [$\sigma_X$]{.blue}. If we let $\bar{X} = \frac{\sum^n_{i=1} X_i}{n}$, then

$$
\bar{X} \rightarrow_D \text{Normal} \! \left(\mu = \mu_X, \sigma = \frac{\sigma_X}{\sqrt{n}}\right)
$$
:::

. . .

::: {style="margin-top: 50px;"}
Similarly, $n \geq 30$ is "good enough" to use a Normally distributed **random variable** to model $\bar{X}$. Furthermore, if $X_i \sim \text{Normal}(\mu_X, \sigma_X)$, then $\bar{X} \sim \text{Normal} \! \left(\mu = \mu_X, \sigma = \frac{\sigma_X}{\sqrt{n}}\right)$.
:::

## Sampling Distribution of the Sample Mean [*in Action*]{.red}

Recall the (virtual) data collection task where we wanted to estimate the *average* weight of these 100 blocks by selecting **only** 10 blocks.

[**URL:** [bit.ly/choc-and-blocks](https://bit.ly/choc-and-blocks)]{style="margin-left: 1em;"}

. . .

::: {style="margin-top: 50px;"}
What percentage of random samples could we expect to have a sample mean weight within *two standard deviations* of the average weight [_**by chance alone**_]{.highlight-blue-revealjs}? [&zigrarr; Approximately 95%]{.printOnly}
:::

## Standard Error of *x&#772;*

The *estimate* of the sampling distribution's *spread* given the **observed data**.

. . .

::: {.definition style="margin-top: 50px; width: 47.5%;"}
Let:

- $\bar{x}$ be the *sample mean*,
- $s$ be the *sample standard deviation*,
- and $n$ be the *sample size*.

Hence, the standard error of $\bar{x}$ is defined as

$$
\text{se}(\bar{x}) = \frac{s}{\sqrt{n}}
$$
:::

::: {.aside}
**Note:** $\bar{x}$ is the *estimate* of the sampling distribution's *centre* given the **observed data**.
:::

## [Ex 4:]{.red} IQ Scores of University Students

<blockquote style="border-left: 0.25em solid #685f5f;">
It was of interest to see whether the mean IQ of university students is the same as that of the population at large, which has a mean IQ of 100. This data comes from 38 subjects selected randomly from a population of university students, and their measured IQ scores have been numerically summarised below.
</blockquote>

$$
\bar{x} = 111.3, \quad s = 22.6, \quad n = 38
$$

::: {.aside}
Data from Sneyd, J., Fewster, R. M., & McGillivray, D. (2022). *Mathematics and Statistics for Science*. Springer.
:::

## Estimation of the Mean

:::::: {.columns}
::::: {.column}
**Point Estimate**

::: {.definition style="width: 2.5%; margin-left: 0;"}
$\bar{x}$
:::

:::: {.fragment}
::: {style="margin-top: 50px;"}
**Interval Estimate^a^**
:::

::: {.definition-orchid style="width: 34%; margin-left: 0;"}
 $\bar{x} \pm t^*_p(\nu) \times \text{se}(\bar{x})$
:::
::::

:::: {.fragment .incremental}
where $t^*_p(\nu)$ is the *t*-multiplier such that:

::: {style="margin-top: -15px;"}
- $p = 1 - (1 - \text{Conf. Level})/2$^b^ 
- $\nu = n - 1$
:::
::::
:::::
::::: {.fragment .column}
Recall for [**Ex 4:**]{.red}
<blockquote style="border-left: 0.25em solid #685f5f; margin-bottom: 50px;">
$$
\bar{x} = 111.3, \quad s = 22.6, \quad n = 38
$$
</blockquote>

```{python IQ-multiplier}
#| echo: true
#| classes: fragment

# Load in the stats submodule from the scipy package
from scipy import stats

# Determine the t-multiplier for a 95% C.I.
stats.t.ppf(0.975, 37)
```
:::::
::::::

::: {.aside}
a. By constructing a *[confidence interval]{.tomato}* for the "true" mean.
b. $\text{Conf. Level}$ is short for *confidence level*.
:::

## Confidence Intervals

> A [_**confidence interval**_]{.tomato} for a parameter is an interval computed from sample data by a method that will capture the parameter for a specified proportion of all samples.
> 
> The success rate (proportion of all samples whose intervals contain the parameter) is known as the _**confidence level**_.

::: {style="font-size: 0.9em; text-align: right; margin-top: -0.5em;"}
 --- Lock et al. (2021)
:::

::: {.aside}
Lock, R. H., Lock P. F., Morgan, K. L., Lock, E. F., & Lock, D. F. (2021). *Statistics: Unlocking the power of data* (3rd ed.). Wiley.
:::

## Student's *t*-distribution

Let $T$ be a *continuous* random variable that follows the *Student's t-distribution*.

::::: {.columns}
:::: {.column}
::: {.fragment .definition style="width: 35%; margin-top: 50px; margin-left: auto !important; margin-right: auto !important;" fragment-index=0}
$$
T \sim \text{Student}(\nu)
$$
:::

::: {.fragment style="margin-top: 50px;" fragment-index=1}
It has a single parameter known as the degrees of freedom, $\nu$, provided that $\nu \in \mathbb{R}_{>0}$.
:::

::: {.fragment style="margin-top: 50px;" fragment-index=2}
The value of a *t*-multiplier is, in fact, the outcome associated with a specific percentile of a Student's *t*-distribution with $\nu$ degrees of freedom.
:::
::::
:::: {.column}
::: {.fragment .panel-tabset fragment-index=1}
#### *&nu;* = 1

```{r Student-1}
#| fig-dpi: 300
#| fig-width: 6
#| fig-height: 5

ggplot() +
  stat_function(fun = dt, xlim = c(-25, 25), geom = "polygon", fill = "darkblue", alpha = 0.2, args = list(df = 1), n = 5000) +
  geom_function(fun = dt, xlim = c(-5, 5), linewidth = 1, colour = "darkblue", args = list(df = 1), n = 5000) +
  theme_bw() +
  labs(x = "x", y = "f(x; θ)") + 
  coord_cartesian(xlim = c(-4, 4))
```

#### *&nu;* = 5

```{r Student-5}
#| fig-dpi: 300
#| fig-width: 6
#| fig-height: 5

ggplot() +
  stat_function(fun = dt, xlim = c(-25, 25), geom = "polygon", fill = "darkblue", alpha = 0.2, args = list(df = 5), n = 5000) +
  geom_function(fun = dt, xlim = c(-5, 5), linewidth = 1, colour = "darkblue", args = list(df = 5), n = 5000) +
  theme_bw() +
  labs(x = "x", y = "f(x; θ)") + 
  coord_cartesian(xlim = c(-4, 4))
```

#### *&nu;* = 25

```{r Student-25}
#| fig-dpi: 300
#| fig-width: 6
#| fig-height: 5

ggplot() +
  stat_function(fun = dt, xlim = c(-25, 25), geom = "polygon", fill = "darkblue", alpha = 0.2, args = list(df = 25), n = 5000) +
  geom_function(fun = dt, xlim = c(-5, 5), linewidth = 1, colour = "darkblue", args = list(df = 25), n = 5000) +
  theme_bw() +
  labs(x = "x", y = "f(x; θ)") + 
  coord_cartesian(xlim = c(-4, 4))
```

#### *&nu;* = &infin;

```{r Student-infin}
#| fig-dpi: 300
#| fig-width: 6
#| fig-height: 5

ggplot() +
  stat_function(fun = dt, xlim = c(-25, 25), geom = "polygon", fill = "darkblue", alpha = 0.2, args = list(df = Inf), n = 5000) +
  geom_function(fun = dt, xlim = c(-5, 5), linewidth = 1, colour = "darkblue", args = list(df = Inf), n = 5000) +
  theme_bw() +
  labs(x = "x", y = "f(x; θ)") + 
  coord_cartesian(xlim = c(-4, 4))
```
:::
::::
:::::

## Interpretation of the Confidence Interval

[**Ex 4:**]{.red} The 95% C.I. for $\mu$ is $[103.8716, 118.7284]$

. . .

::: {style="margin-top: 50px;"}
A good interpretation of a *[confidence interval]{.tomato}* addresses the...
:::

::: {.incremental style="margin-top: -12.5px;"}
- Choice of *confidence level*
- Uncertainty of the quantity estimated
- Units where applicable
:::

::: {.printOnly style="margin-top: 50px;"}
With 95% confidence, we estimate that the true mean IQ score of university students was somewhere between 103.87 and 118.73.
:::

## Test for the Mean<small>1</small>

:::::: {.columns}
::::: {.column}
**Hypothesis Statements^2^**

::: {.definition style="width: 22.5%; margin-left: 0;"}
$H_0\!: \mu = \mu_0$  
$H_1\!: \mu \neq \mu_0$
:::

::: {.fragment}
where $\mu_0$ is the hypothesised value of the "true" mean.
:::

:::: {.fragment}
::: {style="margin-top: 50px;"}
**Test Statistic**
:::

::: {.definition-orchid style="width: 25%; margin-left: 0;"}
$t_0 = \displaystyle \frac{\bar{x} - \mu_0}{ \text{se}(\bar{x})}$
:::
::::
:::::
::::: {.fragment .column}
Recall for [**Ex 4:**]{.red}
<blockquote style="border-left: 0.25em solid #685f5f; margin-bottom: 50px;">
$$
\bar{x} = 111.3, \quad s = 22.6, \quad n = 38
$$
</blockquote>
:::::
::::::

::: {.aside}
1. By conducting a one sample *t*-test, that is, a [*hypothesis test*]{.steelblue} for the "true" mean.
2. By default, we conduct two-sided [*hypothesis tests*]{.steelblue} ($\neq$). A one-sided test can be conducted instead by changing the sign of the *alternative* hypothesis to $<$ or $>$.
:::

## Hypothesis Tests

:::: {.fragment .fragment .semi-fade-out fragment-index=0}
<blockquote style="border-left: 0.25em solid steelblue;">
A [_**statistical (hypothesis) test**_]{.steelblue} is used to determine whether results from a sample are convincing enough to allow us to conclude something about the population.
</blockquote>

::: {style="font-size: 0.9em; text-align: right; margin-top: -0.5em;"}
 --- Lock et al. (2021)
:::
::::

:::: {.columns}
::: {.column .fragment .fade-in-then-semi-out fragment-index=0}
<blockquote style="border-left: 0.25em solid orchid; margin-top: 50px">
The **null hypothesis**, represented by the symbol *H&#8320;*, is a statement that there is "nothing" happening. In most situations, the researcher hopes to disprove or reject the null hypothesis.
</blockquote>
:::
::: {.column .fragment .fade-in-then-semi-out fragment-index=1}
<blockquote style="border-left: 0.25em solid teal; margin-top: 50px">
The **alternative hypothesis**, represented by the symbol *H&#8321;* or *H&#8336;*, is a statement that "something" is happening. In most situations, this hypothesis is what the researcher hopes to prove.
</blockquote>
:::
::::

::: {style="font-size: 0.9em; text-align: right; margin-top: -0.5em;" .fragment fragment-index=0}
 --- Utts & Heckard (2015)
:::

::: {.aside}
Lock, R. H., Lock P. F., Morgan, K. L., Lock, E. F., & Lock, D. F. (2021). *Statistics: Unlocking the power of data* (3rd ed.). Wiley.

Utts, J. M., & Heckard, R. F. (2015). *Mind on statistics* (5th ed.). Cengage Learning.
:::

## Student's *t*-distribution and *p*-values

<blockquote style="border-left: 0.25em solid orange">
The __*p*-value__ (or __*P*-value__) is calculated by assuming that the null hypothesis is true, and then determining the probability of a statistic as extreme as, or more extreme than, the observed statistic
</blockquote>

. . .

::: {style="margin-top: 50px;"}
The test statistic, $t_0$, as defined, describes the distance between the estimate *and* the hypothesised value in **standard error units**.
:::

. . .

::: {style="margin-top: 50px;"}
The sampling distribution of $t_0$ is modelled by a Student's *t*-distribution with $\nu = n-1$ degrees of freedom, [given that the **_null_ is true**]{.fragment .highlight-red}.
:::

. . .

::: {style="margin-top: 50px;"}
Hence, if a "small" *p*-value is calculated based on the observed $t_0$: [We essentially say our data is incompatible with the *null*, and, therefore, we reject it.]{.printOnly}
:::

::: {.aside}
**Note:** A "large" *p*-value does not mean we can accept the *null*. One reason for this is that one component of $t_0$ is a value determined<br>[**Note:**]{style="visibility: hidden;"} by us and not by the observed data.
:::

## Rejection Regions and *p*-values

What is "small enough" for a *p*-value? We can set a (significance) level, $\alpha\%$, to explain the outcome of the [hypothesis test]{.steelblue}.

:::: {.columns}
::: {.fragment .column width="33.33%"}
```{r ER-1}
#| fig-dpi: 300
#| out-width: 100%
#| fig-width: 8
#| fig-height: 7
#| fig-cap: <strong>Figure&colon;</strong> $\mathbb{P}(\mu - \sigma < X < \mu + \sigma) \approx 0.6827$.

ER <- \(z) {
  par(mai = c(0.6, 0.65, 0, 0))
  z0 <- seq(-4, 4, length.out = 1000)
  plot(0, type = "n", xlim = c(-3, 3), ylim = c(0, dnorm(0) * 1.1), axes = FALSE, yaxs = "i", xlab = "", ylab = "")
  lines(x = z0, y = dnorm(z0), col = "gray")
  abline(h = 0)
  axis(1, tcl = -0.2, mgp = c(0.75, 0.5, 0)); mtext("x", side = 1, line = 1.7, font = 2)
  axis(2, tcl = -0.2, mgp = c(0.75, 0.5, 0), las = 2, at = c(0, 0.1, 0.2, 0.3, 0.4, 1.0)); mtext("f(x; μ, σ)", side = 2, line = 2.25, font = 2)
  
  zz <- seq(-z, z, length.out = 1000)
  
  polygon(x = c(zz, rev(zz)), y = c(dnorm(zz), rep(0, length(zz))), col = "yellow", lwd = 2)
}

ER(1)
```
:::
::: {.fragment width="33.33%" .column}
```{r ER-2}
#| fig-dpi: 300
#| out-width: 100%
#| fig-width: 8
#| fig-height: 7
#| fig-cap: <strong>Figure&colon;</strong> $\mathbb{P}(\mu - 2\sigma < X < \mu + 2\sigma) \approx 0.9545$.

ER(2)
```
:::
::: {.fragment width="33.33%" .column}
```{r ER-3}
#| fig-dpi: 300
#| out-width: 100%
#| fig-width: 8
#| fig-height: 7
#| fig-cap: <strong>Figure&colon;</strong> $\mathbb{P}(\mu - 3\sigma < X < \mu + 3\sigma) \approx 0.9973$.

ER(3)
```
:::
::::

. . .

These test statistic "thresholds", determined by $\alpha$, can also be found using *t*-multipliers. When a *t*-multiplier is used in this fashion, it is instead known as a **critical value**.

## Using Student's *t*-distribution to Calculate the *p*-value

[**Ex 4:**]{.red} $t_0 = 3.082$ for $H_0\!: \mu = 100$ versus $H_1\!: \mu \neq 100$.

. . .

::: {style="margin-bottom: 50px;"}
Under the *null*, $T \sim \text{Student}(\nu)$, where $\nu = n - 1$.
:::

::::: {.columns style="font-size: 0.9em;"}
:::: {.column .fragment}
::: {.definition style="width: 85%; margin-left: 0;"}
If it is a two-sided test, e.g. $H_1 \! : \mu \neq \mu_0$

$\quad p\text{-value} = 2 \times \mathbb{P}(T > |t_0|)$

$\quad \text{Critical value, } t, ~ \text{fulfills:} ~ \mathbb{P}(T > |t|) = \alpha/2$
:::

::: {.definition-orchid .fragment style="margin-top: 25px; width: 85%; margin-left: 0;"}
If it is a one-sided test and $H_1 \! : \mu > \mu_0$

$\quad p\text{-value} = \mathbb{P}(T > t_0)$

$\quad \text{Critical value, } t, ~ \text{fulfills:} ~ \mathbb{P}(T > t) = \alpha$
:::

::: {.definition-teal .fragment style="margin-top: 25px; width: 85%; margin-left: 0;"}
If it is a one-sided test and $H_1 \! : \mu < \mu_0$

$\quad p\text{-value} = \mathbb{P}(T < t_0)$

$\quad \text{Critical value, } t, ~ \text{fulfills:} ~ \mathbb{P}(T < t) = \alpha$
:::
::::
:::: {.column}
```{python IQ-pvalue}
#| echo: true
#| classes: fragment

# Load in the stats submodule from the scipy package
from scipy import stats

# Calculate the p-value using the "percentile" approach
2 * (1 - stats.t.cdf(abs(3.082), 37))

# Determine the critical value at the 5% level
stats.t.ppf(0.975, 37)
```
::::
:::::

## Explanation of the Outcome of the Hypothesis Test

[**Ex 4:**]{.red} $t_0 = 3.082$ for $H_0\!: \mu = 100$ versus $H_1\!: \mu \neq 100$.

The *p*-value was 0.0039 and the critical value was 2.026.

. . .

::: {style="margin-top: 50px;"}
A good explanation of the outcome of the *[hypothesis test]{.steelblue}* includes...
:::

::: {.incremental style="margin-top: -12.5px;"}
- Contextualisation of the hypothesis statements
- The choice of level, *&alpha;*, used to quantify a "small enough" *p*-value
- A quote of the *p*-value
:::

::: {.printOnly style="margin-top: 50px;"}
We do reject the null that the true mean IQ score of university students was 100 in favour of the alternative that they are not 100 at the 5% level (*p*-value = 0.0039). 
:::

## [Ex 5:]{.red} Cholesterol Levels

::::: {.columns}
:::: {.column width="70%"}
<blockquote style="border-left: 0.25em solid #685f5f;">
The `cholest.xlsx` file describes a random sample of patients used to estimate the average cholesterol level, which is expected to be 200 mg/dL. This is because these patients are being assessed to be part of a control group for a study on heart attacks.
</blockquote>

::: {.fragment fragment-index=1 .orchid-table style="font-size: 0.9em; margin-top: 50px; width: 90%;"}
+:----------+:----------------------------------------------------------------------------------------------------------------------------+
| **level** | A number that denotes the cholesterol level of a patient, measured in milligrams per decilitre (mg/dL). 
+-----------+-----------------------------------------------------------------------------------------------------------------------------+
:::
::::
:::: {.column width="30%" .fragment fragment-index=0}
```{r cholest} 
#| results: asis
#| classes: excel

library(readxl)
cholest.df <- read_excel("datasets/cholest.xlsx")
 
rbind(colnames(cholest.df), head(cholest.df, n = 14), rep("...", ncol(cholest.df))) |>
  kable(format = "html", row.names = TRUE, col.names = LETTERS[1:ncol(cholest.df)])
```
::::
:::::

::: {.aside}
Data from Utts, J. M. & Heckard, R. F. (2015). *Mind on Statistics* (5th ed.). Cengage.
:::

## Assumption for Inference on the Mean

:::::: {.columns}
::::: {.column width="45%"}
```{r cholest-dotplot}
#| fig-dpi: 300
#| fig-height: 3.5
#| fig-width: 7
#| out-width: 90%
#| fig-align: center
#| fig-cap: "**Figure:** Distribution of 30 participants' cholesterol levels."

ggplot(cholest.df, aes(x = level)) +
  geom_dotplot(fill = NA, stroke = 3, colour = "black", binwidth = (max(cholest.df$level) - min(cholest.df$level)) / 25, dotsize = 0.7) + 
  scale_y_continuous(breaks = NULL) +
  labs(y = NULL, x = "Cholesterol Level (mg/dL)", title = "Distribution of Cholesterol Levels") +
  theme_bw()
```
:::::
::::: {.column width="55%"}
<ol>
  <li class="fragment" style="line-height: 1.1;"> Observations are independent.</li>
  <li class="fragment" style="line-height: 1.1;"> One measure of centre is an appropriate summary of the data.</li>
  <li class="fragment" style="line-height: 1.1;"> The data is approximately symmetrical about its sample mean.</li>
  <li class="fragment" style="line-height: 1.1;"> No outliers.</li>
</ol>

:::: {.fragment}
::: {style="margin-top: 50px; border-bottom: 5px solid orange; width: 40%;"}
3. and 4. in practice: 
:::

<ul>
  <li class="fragment" style="line-height: 1.1;"> Follow strictly for "[small]{.tomato}" datasets (*n* < *20*).</li>
  <li class="fragment" style="line-height: 1.1;"> Lenient **if and only if** se(*x&#772;*) is not heavily affected for "[medium]{.steelblue}" datasets (*20* &leq; *n* &leq; *50*).</li>
  <li class="fragment" style="line-height: 1.1;"> **Incredibly** lenient for "[large]{.orchid}" datasets (*n* > *50*).</li>
</ul>
::::
:::::
::::::

## [Ex 5]{.red} Inferential Statistics in [Python]{.cornflowerblue}

::: {style="width: 85%;"}
```{python Ex-5}
#| echo: true

# Load in one submodule from the scipy package and the pandas package
from scipy import stats
import pandas

# Read in the cholest.xlsx file
cholest_df = pandas.read_excel("datasets/cholest.xlsx")

# Calculate the inferential statistics
cholest_ttest = stats.ttest_1samp(
  a = cholest_df["level"],
  popmean = 200,            # The hypothesised value for the true mean
  alternative = "two-sided" # Other values this argument can take are "less" and "greater"
)

# Print the constructed 95% confidence interval
cholest_ttest.confidence_interval(confidence_level = 0.95)

# Print the result of the hypothesis test, whose alternative hypothesis was specified above
cholest_ttest
```
:::

::: {.aside}
**Documentation**: [https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_1samp.html#scipy.stats.ttest_1samp](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_1samp.html#scipy.stats.ttest_1samp)
:::

## Linking *Two-sided* Tests and Confidence Intervals

Commonly, a two-sided [*hypothesis test*]{.steelblue} for a parameter at the [*&alpha;*%]{.orchid} level *and* a [(1 &minus; *&alpha;*)%]{.teal} [*confidence interval*]{.tomato} for the same parameter will lead to similar conclusions (based on the observed data).

. . .

::: {style="margin-top: 50px"}
Recall for [**Ex 5**]{.red} that we are tested $H_0\!: \mu = 200$ versus $H_1\!: \mu \neq 200$, and the *p*-value was [0.1024]{.blue}.
:::

:::: {.fragment .fade-in-then-semi-out style="margin-left: 25px; font-size: 0.9em;"}
Suppose we explained the outcome of the [*hypothesis test*]{.steelblue} at the 20% level. [&zigrarr; Do reject...]{.printOnly}

::: {style="width: 65%;"}
```{python Ex-5a}
#| echo: true
cholest_ttest.confidence_interval(confidence_level = 0.80)
```
:::
::::

:::: {.fragment .fade-in-then-semi-out style="margin-left: 25px; margin-top: 25px; font-size: 0.9em;"}
Suppose we explained the outcome of the [*hypothesis test*]{.steelblue} at the 5% level. [&zigrarr; Fail to reject...]{.printOnly}

::: {style="width: 65%;"}
```{python Ex-5b}
#| echo: true
cholest_ttest.confidence_interval(confidence_level = 0.95)
```
:::
::::

:::: {.fragment style="margin-left: 25px; margin-top: 25px; font-size: 0.9em;"}
Suppose we explained the outcome of the [*hypothesis test*]{.steelblue} at the 1% level. [&zigrarr; Fail to reject...]{.printOnly}

::: {style="width: 65%;"}
```{python Ex-5c}
#| echo: true
cholest_ttest.confidence_interval(confidence_level = 0.99)
```
:::
::::

::: {.aside}
**Note:** A one-sided [*hypothesis test*]{.steelblue} for a parameter at the [*&alpha;*%]{.orchid} level *does not* have an equivalent  relationship with [(1 &minus; *&alpha;*)%]{.teal} [*confidence*]{.tomato}<br>[**Note:**]{style="visibility: hidden;"} [*interval*]{.tomato} for the same parameter due to the nature of a one-sided test's rejection region.
:::

## Practice Exercises {visibility="uncounted"}

<ol style="line-height: 1.1; font-size: 0.8em;">
  <li style="margin-bottom: 50pt;">
    <ol>
      <li style="list-style-type: lower-alpha; line-height: 1.1; font-size: 1em;">
      Interpret the 95% confidence interval constructed for [**Ex 5**]{.red}.
      </li>
      <li style="list-style-type: lower-alpha; line-height: 1.1; font-size: 1em;">
      Explain the outcome of the hypothesis test conducted for [**Ex 5**]{.red} at the 5% level.
      </li>
    </ol>
  </li>
  
  <li style="margin-bottom: 50pt;">
  Suppose we plotted data typically analysed when conducting inference for a mean setting. Which assumptions for inference can be checked with the plot?
  </li>
  
  <li>
  A chemist has developed a new polymer for creating a high-tensile strength string. He wants to test whether the average weight that this new string can suspend before the string breaks (breaking strength) is different from the breaking strength of the current string used for a particular application, in which the current string has a mean breaking strength of 5.8 kilograms. 
  
  He made ten string samples, recorded the breaking strength (measured in kilograms) and presented them below.
$$
5.73, \quad 6.21, \quad 5.85, \quad  5.92, \quad  5.97, \quad 6.15, \quad  6.05, \quad  5.87, \quad  5.91, \quad  5.88
$$
  You can answer the following questions as if the assumptions for inference have been met.
    <ol>
      <li style="list-style-type: lower-alpha; line-height: 1.1;">
      Construct a 95% confidence interval for the mean breaking strength.
      </li>
      <li style="list-style-type: lower-alpha; line-height: 1.1;">
      Interpret the 95% confidence interval.
      </li>
      <li style="list-style-type: lower-alpha; line-height: 1.1;">
      State the null *and* alternative hypothesis statements that can be used to answer the chemist's question.
      </li>
      <li style="list-style-type: lower-alpha; line-height: 1.1;">
      Based on your hypotheses, calculate the test statistic *and* determine the critical value.
      </li>
      <li style="list-style-type: lower-alpha; line-height: 1.1;">
      Explain the outcome of the hypothesis test conducted at the 5% level.
      </li>
      <li style="list-style-type: lower-alpha; line-height: 1.1;">
      Based on your answers above, make a conclusion on whether the breaking strength of the new high-tensile strength string is different from the current string.
      </li>
    </ol>
  </li>
</ol>



