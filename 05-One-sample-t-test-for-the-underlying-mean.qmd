---
title: "One-sample *t*-test for the Underlying Mean"
author: "ENGEN102-23B (HAM) & (SEC) - Engineering Maths and Modelling 1B"
format:
  revealjs: 
    slide-number: c/t
    progress: true
    auto-stretch: false
    height: 1080
    width: 1920
    theme: reveal-extra/quartz.scss
    template-partials:
      - reveal-extra/title-block.html
    highlight-style: ayu
    code-line-numbers: false
    embed-resources: true
editor: source
---

## Hypothesis testing {.margin-bottom}

Another common statistical inference method is a hypothesis test^[Also known as a significance test]

. . .

This method allows us to examine whether the observed data provide evidence to refute a hypothesised value for a parameter, e.g. the underlying mean of a random variable, in favour of an "alternative"

## [Definition:]{style="color: darkorange;"} The hypothesis statements

:::: {.columns}
::: {.column width="49%"}
::: {style="padding: 0.1em 0.5em 0.1em; font-size: 0.8em; border-left: 0.25em solid tomato; color: #4d4d4d; line-height: 1;"}
The **null hypothesis**, represented by the symbol *H~0~*, is a statement that there is "nothing" happening. In most situations, the researcher hopes to disprove or reject the null hypothesis
:::
:::
::: {.column .fragment width="51%" fragment-index=1}
::: {style="padding: 0.1em 0.5em 0.1em; font-size: 0.8em; border-left: 0.25em solid steelblue; color: #4d4d4d; line-height: 1;"}
The **alternative hypothesis**, represented by the symbol *H~1~* or *H~a~*, is a statement that "something" is happening. In most situations, this hypothesis is what the researcher hopes to prove
:::
:::
::::

::: {style="font-size: 0.8em; text-align: right; margin-top: -0.5em;" .fragment fragment-index=1}
 --- Utts & Heckard (2015)
:::

::: {.aside}
Utts, J. M., & Heckard, R. F. (2015). *Mind on statistics* (5th ed.). Cengage Learning.
:::

## [Definition:]{style="color: darkorange;"} The test statistic for *&mu;*

::: {.definition style="width: 40%;"}
$$
t_0 = \frac{\bar{x} - \mu_0}{\text{se}(\bar{x})}
$$
:::

. . .

where:

::: {.incremental style="font-size: 0.9em; margin-top: -0.25em;"}
- $t_0$ is the *T*-test statistic (for $\mu$)
- $\bar{x}$ is the *sample* mean
- $\mu_0$ is the **hypothesised** value of the *underlying* mean
- $\text{se}(\bar{x})$ is the **standard error** of $\bar{x}$
:::

## ^[What the hypothesis test for *&mu;* is more commonly known as]One-sample *t*-test for *&mu;* {.margin-bottom}

The measure of evidence against the null is presented as a *p*-value[. In this scenario: A probability of observing a test statistic as extreme as, or more extreme than, the observed test statistic, assuming that the null hypothesis is true]{.fragment}

. . .

:::: {.columns}
::: {.column}
A one-sample t-test uses the Student's t-distribution^[See [Wikipedia](https://en.wikipedia.org/wiki/Student%27s_t-distribution)], which has a parameter $\nu$ to calculate this probability
:::
::: {.column}
```{r}
#| fig-dpi: 300
#| fig-width: 7
#| fig-height: 7
#| out-width: "60%"
#| fig-align: center
par(mai = c(0.5, 0.55, 0.15, 0.15), cex.axis = 0.9, mgp = c(1.5, 0.5, 0))
curve(dt(x, df = 10), from = -4, to = 4, xlim = c(-3, 3), lwd = 2, xlab = "t", ylab = bquote(f[T](t~";"~nu)), las = 1, tcl = -0.35, col = 1)
abline(h = 0, col = "lightgray")

curve(dt(x, df = 5), from = -4, to = 4, lwd = 2, add = TRUE, col = 2)

curve(dt(x, df = 100), from = -4, to = 4, lwd = 2, add = TRUE, col = 4)

legend("topright", legend = c(bquote(nu==5), bquote(nu==10), bquote(nu==100)), bty = "n", col = c(2, 1, 4), lty = 1, lwd = 2)

box()
```
:::
::::

## Calculation of the *p*-value

Let $T \sim \text{Student}(\nu=n-1)$, where:

::: {.tightList style="font-size: 0.9em;"}
- $\nu$ is the Student’s *t*-distribution’s degrees of freedom parameter
- $n$ is the sample size
- $t_0$ is the *T*-test statistic
:::

::::: {.columns style="font-size: 0.75em;"}
:::: {.column .fragment width="45%"}
::: {.definition}
If it is a two-sided test, e.g. [$H_1 \! : \mu \neq \mu_0$]{style="font-size: 0.9em"}

$\quad p\text{-value} = 2 \times \mathbb{P}(T > |t_0|)$
:::

:::: {style="margin-top: 25px;"}
::: {.definition-orchid .fragment}
If it is a one-sided test and [$H_1 \! : \mu > \mu_0$]{style="font-size: 0.9em"}

$\quad p\text{-value} = \mathbb{P}(T > t_0)$
:::
::::

:::: {style="margin-top: 25px;"}
::: {.definition-teal .fragment}
If it is a one-sided test and [$H_1 \! : \mu < \mu_0$]{style="font-size: 0.9em"}

$\quad p\text{-value} = \mathbb{P}(T < t_0)$
:::
::::
::::
:::::

## Rejection regions for *t*-tests

Rather than approximating *p*-values by hand, we use the rejection region approach to determine the outcome of the hypothesis tests in ENGEN102

::: {style="margin-top: 25px;"}
:::

:::::: {.columns}
::::: {.column .fragment style="font-size: 0.75em; width: 45%;" fragment-index=0}
::: {.definition}
If it is a two-sided test, e.g. [$H_1 \! : \mu \neq \mu_0$]{style="font-size: 0.9em"}

$\quad |t_0| > t^*_{1-\alpha/2}(\nu) \Rightarrow p\,\text{-value} < \alpha$
:::

:::: {style="margin-top: 25px;"}
::: {.definition-orchid .fragment fragment-index=1}
If it is a one-sided test and [$H_1 \! : \mu > \mu_0$]{style="font-size: 0.9em"}

$\quad t_0 > t^*_{1-\alpha}(\nu) \Rightarrow p\,\text{-value} < \alpha$
:::
::::

:::: {style="margin-top: 25px;"}
::: {.definition-teal .fragment fragment-index=2}
If it is a one-sided test and [$H_1 \! : \mu < \mu_0$]{style="font-size: 0.9em"}

$\quad t_0 < t^*_{\alpha}(\nu) \Rightarrow p\,\text{-value} < \alpha$
:::
::::
:::::
::::: {.column style="width: 55%;"}
:::: {.r-stack}
::: {.fragment fragment-index=0 .fade-in-then-out}
```{r}
#| fig-dpi: 300
#| fig-align: center
par(mai = c(0.5, 0.55, 0.15, 0.15), cex.axis = 0.9, mgp = c(1.5, 0.5, 0))
curve(dt(x, df = 10), from = -4, to = 4, xlim = c(-3, 3), lwd = 2, xlab = "t", ylab = bquote(f[T](t~";"~nu==10)), las = 1, tcl = -0.35)
abline(h = 0, col = "lightgray")

tstar <- seq(qt(0.975, df = 10), 4, length.out = 100)
polygon(c(tstar, rev(tstar)), c(dt(tstar, df = 10), rep(0, length(tstar))), col = "gold")
tstar <- seq(qt(0.025, df = 10), -4, length.out = 100)
polygon(c(tstar, rev(tstar)), c(dt(tstar, df = 10), rep(0, length(tstar))), col = "gold")

text(x = c(-2.75, 2.75), y = 0.05, labels = bquote(alpha/2), cex = 1.5)

text(x = 0, y = 0.25, labels = bquote(1-alpha), cex = 1.5)

curve(dt(x, df = 10), from = -4, to = 4, lwd = 2, add = TRUE)

box()
```
:::
::: {.fragment fragment-index=1 .fade-in-then-out}
```{r}
#| fig-dpi: 300
#| fig-align: center
par(mai = c(0.5, 0.55, 0.15, 0.15), cex.axis = 0.9, mgp = c(1.5, 0.5, 0))
curve(dt(x, df = 10), from = -4, to = 4, xlim = c(-3, 3), lwd = 2, xlab = "t", ylab = bquote(f[T](t~";"~nu==10)), las = 1, tcl = -0.35)
abline(h = 0, col = "lightgray")

tstar <- seq(qt(0.95, df = 10), 4, length.out = 100)
polygon(c(tstar, rev(tstar)), c(dt(tstar, df = 10), rep(0, length(tstar))), col = "plum")

text(x = 2.75, y = 0.05, labels = bquote(alpha), cex = 1.5)

text(x = 0, y = 0.25, labels = bquote(1-alpha), cex = 1.5)

curve(dt(x, df = 10), from = -4, to = 4, lwd = 2, add = TRUE)

box()
```
:::
::: {.fragment fragment-index=2}
```{r}
#| fig-dpi: 300
#| fig-align: center
par(mai = c(0.5, 0.55, 0.15, 0.15), cex.axis = 0.9, mgp = c(1.5, 0.5, 0))
curve(dt(x, df = 10), from = -4, to = 4, xlim = c(-3, 3), lwd = 2, xlab = "t", ylab = bquote(f[T](t~";"~nu==10)), las = 1, tcl = -0.35)
abline(h = 0, col = "lightgray")

tstar <- seq(qt(0.05, df = 10), -4, length.out = 100)
polygon(c(tstar, rev(tstar)), c(dt(tstar, df = 10), rep(0, length(tstar))), col = "aquamarine3")

text(x = -2.75, y = 0.05, labels = bquote(alpha), cex = 1.5)

text(x = 0, y = 0.25, labels = bquote(1-alpha), cex = 1.5)

curve(dt(x, df = 10), from = -4, to = 4, lwd = 2, add = TRUE)

box()
```
:::
::::
:::::
::::::

. . .

::: {style="margin-top: -37.5px;"}
:::

When we use the *t*-multiplier in this fashion, it is more formally known as the critical value of the hypothesis test

## [Ex 6:]{style='color: tomato;'} Replication with light speeds

> Simon Newcomb experimented with a new method of measuring the speed of light in 1882, which involved using two different mirrors. The theoretical passage time for the above distance was 24.8296 millionths of a second. If this new method is unbiased and precise, the experimental data should agree with the theoretical passage time, which has been summarised below
$$\bar{x} = 24.8285, \quad\quad s = 0.0051, \quad\quad n = 20$$
Conduct a hypothesis test at the 5% level to evaluate whether this new method agrees with the theoretical passage time. Then interpret the outcome of this test at the 5% level.

. . .

::: {style="width: 55%; margin-top: 25px;"}
```{python}
#| echo: true
# A Python script to print the exact value of the t-multiplier
from scipy.stats import t
t.ppf(0.975, df=19)
```
:::


[-0.9646]{style="color: ghostwhite;"}

## Assumptions for inference on *&mu;*

1. **Independent** observations
2. One measure of centre can satisfactorily summarise the data
3. Approximately symmetrical about the **sample mean**, $\bar{x}$, and there are no outliers

. . .

::: {style="border-bottom: 5px solid orange; width: 12.5%;"}
More on **1.**
:::

. . .

In practice, this is typically met when we...

::: {.incremental style="margin-top: -0.25em;"}
- ... take a *random* sample from a population
- ... carry out an experiment that uses *randomisation* to eliminate potential biases<br>[...&nbsp;]{style="visibility: hidden;"}in the data collection
:::

## [Ex 3]{style='color: tomato;'} in [iNZight]{.R}

:::::: {.columns}
::::: {.column style="width: 50%;"}
:::: {.r-stack}
::: {.fragment .fade-out fragment-index=1}
```{r}
#| echo: false
#| classes: output-small-80
library(iNZightPlots)
railway.df <- read.csv("datasets/final-crack-depth.csv")
inzsummary(~ depth, data = railway.df, width = 83)
```

```{python}
#| echo: true
# A Python script to print the exact value of the t-multiplier
from scipy.stats import t
t.ppf(0.975, df=249)
```

:::
::: {.fragment fragment-index=1}
```{r}
#| echo: false
#| classes: output-small-80
inzsummary(~ depth, data = railway.df, width = 83, summary.type = "inference", inference.type = "conf", hypothesis.value = 5)
```
:::
::::
:::::

::::: {.column .margin-bottom style="width: 50%;"}
We know that the engineers used railway axles with an initial crack depth of 5 millimetres [128.12]{style="color: ghostwhite;"}

:::: {.fragment fragment-index=0}
**Assumptions**  
[It is unclear whether independence has been met for [Ex 3]{style="color: tomato;"}. Otherwise, the other two assumptions, a single measure of centre and approximately symmetrical about $\bar{x}$, are met. Note that the latter assumption has been met because of the large sample size.]{style="font-size: 0.9em;"}
::::

:::: {.fragment fragment-index=2}
**Interpretation**  
[We reject the null that the underlying mean final crack depth is 5 millimetres, in favour of the alternative that it is not, at the 5% level]{style="font-size: 0.9em;"}
::::
:::::
::::::

## Why the "double negative"? 

::: {.incremental style="margin-left: 1em;"}
1. We **hypothesised** a value for the underlying mean $\mu$
2. We **assumed** the null hypothesis was true to calculate a *p*-value *and* the rejection region
3. We **could have** hacked a hypothesised value to have the inference agree with our question of interest
:::
