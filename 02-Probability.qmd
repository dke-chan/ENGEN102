---
title: "Probability"
subtitle: "_**with a focus on the Normal Distribution and Central Limit Theorem**_"
author: "ENGEN102-23G (HAM) - Engineering Maths and Modelling 1B"
format:
  revealjs: 
    slide-number: c/t
    progress: true
    auto-stretch: false
    height: 1080
    width: 1920
    theme: reveal-extra/quartz.scss
    template-partials:
      - reveal-extra/title-block.html
    highlight-style: breeze
    code-line-numbers: false
    embed-resources: false
editor: source
---

```{r setup}
#| include: false

```

## Outcomes[, events]{.fragment fragment-index=1}[, and probabilities]{.fragment fragment-index=2}

::::: {.columns}
:::: {.column width="65%"}
An [outcome]{.tomato} is a *possible* value we can observe.

<blockquote>
The outcomes of rolling a fair six-sided dice are 1, 2, 3, 4, 5, and 6.
</blockquote>

::: {.fragment fragment-index=1 style="margin-top: 50px;"}
An [event]{.steelblue} is a *set of distinct* [outcome(s)]{.tomato} we can observe.

<blockquote style="border-left: 0.25em solid steelblue;">
The event that I rolled an odd number with a fair six-sided dice consists of the outcomes 1, 3, and 5.
</blockquote>
:::

::: {.fragment fragment-index=2 style="margin-top: 50px; padding-right: 4pt;"}
A [probability]{.orchid} is a *real number*, $\mathbb{R}$, that takes a value between 0 and 1 **inclusive**, which describes the occurrence of an [event]{.steelblue} based on the "long-run" frequency of the [event]{.steelblue}.

<blockquote style="border-left: 0.25em solid orchid;">
The probability of the event that I rolled an odd number with a fair six-sided dice is 0.5.
</blockquote>
:::
::::
:::: {.column width="35%"}
![**Figure:** Two six-sided dice by [glitch](https://openclipart.org/detail/210239/misc-dice) (CC0 1.0).](figures/misc-dice.svg)
::::
:::::

## Random Variables

In Statistics, a **random variable** summarises the [outcomes]{.tomato} (represented as numeric values) of a process by [_**chance alone**_]{.fragment .highlight-blue}.

::: {.fragment style="margin-top: 50px;"}
It is a function whose *domain* is all possible distinct [outcomes]{.tomato} we can observe from a process, and each [outcome]{.tomato} is mapped to a *non-negative* real number, $\mathbb{R}_{\geq0}$, according to the probability axioms^[See the [Wikipedia](https://en.wikipedia.org/wiki/Probability_axioms) page on the probability axioms you are interested in what they are.].
:::

::: {.fragment style="margin-top: 50px;"}
This is known as the **probability distribution** (that the random variable follows). [As a result, it is standard to denote a random variable with uppercase letters [*X*]{.MathJax-Font}, [*Y*]{.MathJax-Font}, etc. and its *domain* with lowercase letters [*x*]{.MathJax-Font}, [*y*]{.MathJax-Font}, etc.]{.fragment}
:::

## More on Random Variables

A **random variable** can be used to model the [outcomes]{.tomato} we could observe from a process by _**chance alone**_. [But why do we assign a *non-negative* real number to [outcomes]{.tomato}?]{.fragment}

:::::: {.fragment style="margin-top: 50px;"}
::::: {.columns}
:::: {.column width="55%"}
Consider the following cases:

::: {.li-line-height-1-1 style="padding-right: 24pt;"}
1. A fair six-sided die, whose [outcomes]{.tomato} are 1, 2, 3, 4, 5, and 6.
2. A uniform random number generator, whose [outcomes]{.tomato} are any real number between 1<br>and 6 inclusive.
:::
::::
:::: {.column width="45%"}
```{python random-numbers}
#| echo: true
#| class: fragment

# Load in the random package
import random

# One "simulated" die roll
print(random.randint(1, 6))

# One uniform random number
print(random.uniform(1, 6))
```
::::
:::::
::::::

::: {.fragment style="margin-top: 50px;"}
*Case 1* is an example of what is known as a *discrete* random variable, whereas *Case 2* is an example of what is known as a *continuous* random variable. 
:::

::: {.fragment style="margin-top: 50px;"}
The key takeaway is that we should _**always**_ assign [probabilities]{.orchid} to [events]{.steelblue}, *regardless* of the type of random variable.
:::

## Normal Distribution

Let $X$ be a *continuous* random variable that follows the *Normal distribution*.

::: {.fragment .definition style="width: 25%;"}
$$
X \sim \text{Normal}(\mu, \sigma)
$$
:::

:::: {.fragment style="margin-top: 50px;"}
The **probability distribution** of a Normally distributed random variable is determined by

::: {.definition-orchid style="width: 55%; display: block; margin: auto;"}
$$
f_X(x; \mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-(x-\mu)^2/2\sigma^2}, ~ \forall ~ x \in \mathbb{R}.
$$
:::

::: {style="margin-top: 25px;"}
where $\mu$ is the mean of the distribution and $\sigma$ is the standard deviation of the distribution, provided that $\mu \in \mathbb{R}$ and $\sigma \in \mathbb{R}_{>0}$.
:::
::::

::: {.aside}
**Note:** The *Normal distribution* is also known as the *Gaussian distribution*
:::

## [Ex 2:]{.red} Unwanted Moles

::::: {.columns}
:::: {.column width="70%"}
<blockquote style="border-left: 0.25em solid #685f5f;">
Stewart (1994) reported on data collected by a company that removed unwanted moles. It was found that the weights of the moles were approximately Normally distributed, with a mean of 150 grams and a standard deviation of 56 grams.
</blockquote>

::: {.fragment style="margin-top: 50px;"}
What process could describe how we can observe the weights of moles by _**chance alone**_? [Ideally, a random sample.]{.printOnly}
:::

::::
:::: {.column width="30%"}
![**Figure:** Mole by Amethyst Studio from<br>[**Figure:**]{style="visibility: hidden;"} [Noun Project](https://thenounproject.com/browse/icons/term/mole/) (CC BY 3.0).](figures/noun-mole.svg)
::::
:::::

::: {.aside}
Stewart, Ian (17 September 1994). Statistical modelling. *New Scientist: Inside Science*, **74**, 14.
:::

## Events with Normal Random Variables

:::::: {.columns}
::::: {.column  width="70%"}
For [**Ex 2**]{.red}, we let $X \sim \text{Normal}(\mu = 150, \sigma = 56)$ and note that the units are in grams.

:::: {.fragment style="margin-top: 50px;" fragment-index=0}
An [event]{.steelblue} for *any* Normal random variable involves an interval and, in rare cases, *disjoint* intervals. [For example:]{.fragment fragment-index=1}

::: {.fragment fragment-index=1}
- A mole has a weight that is between 150 to 206 grams.
- A mole has a weight that is lower than 100 grams.
- A mole has a weight that is 125 &pm; 0.5 grams.
:::
::::

:::::
::::: {.column width="30%"}
```{r mole-norm}
#| fig-dpi: 300
#| out-width: 100%
#| fig-align: center
#| fig-width: 8
#| fig-height: 7
#| fig-cap: "**Figure:** A Normal random variable with<br>[**Figure:**]{style=\"visibility: hidden\"} [*&mu;* = 150]{.MathJax-Font} and [*&sigma;* = 56]{.MathJax-Font}."

par(mai = c(0.6, 0.65, 0, 0))
curve(dnorm(x, 150, 56), type = "n", xlim = 150 + c(-3, 3) * 56, ylim = c(0, dnorm(150, 150, 56) * 1.1), axes = FALSE, yaxs = "i", xlab = "", ylab = "")
abline(h = 0)
axis(1, tcl = -0.2, mgp = c(0.75, 0.5, 0))
mtext("x", side = 1, line = 1.7, font = 2)
axis(2, tcl = -0.2, mgp = c(0.75, 0.5, 0), las = 2, cex.axis = 0.75, at = c(seq(0, 0.007, length.out = 8), 1))
mtext("f(x; 150, 56)", side = 2, line = 2.25, font = 2)
curve(dnorm(x, 150, 56), lwd = 2, add = TRUE, from = 150 - 4 * 56, to = 150 + 4 * 56, col = "darkblue", n = 501)
```
:::::
::::::

## Probability Statements with Normal Random Variables

::::: {.columns}
:::: {.column  width="70%"}
In Statistics, we use the $\mathbb{P}(\cdot)$ notation to denote the [probability]{.orchid} of an [event]{.steelblue}.

Let's assume for [**Ex 2**]{.red} that a random sample is the process observing the weights of moles:

::: {.incremental .li-line-height-1-1}
- The [probability]{.orchid} of a randomly selected mole has a weight that is between 150 to 206 grams. [&zigrarr; $\mathbb{P}(150 < X < 206)$]{.printOnly}
- The [probability]{.orchid} of a randomly selected mole has a weight that is lower than 100 grams. [&zigrarr; $\mathbb{P}(X < 100)$]{.printOnly}
- The [probability]{.orchid} of a randomly selected mole has a weight that is 125 &pm; 0.5 grams. [&zigrarr; $\mathbb{P}(124.5 < X < 125.5)$]{.printOnly}

:::
::::
:::: {.column  width="30%"}
```{r mole-norm}
#| fig-dpi: 300
#| out-width: 100%
#| fig-align: center
#| fig-width: 8
#| fig-height: 7
#| fig-cap: "**Figure:** A Normal random variable with<br>[**Figure:**]{style=\"visibility: hidden\"} [*&mu;* = 150]{.MathJax-Font} and [*&sigma;* = 56]{.MathJax-Font}."
```
::::
:::::

## Calculation of Probabilities in [Python]{.cornflowerblue .fragment fragment-index=0}

::::: {.columns}
:::: {.column width="67.5%"}
::: {style="padding-right: 24pt;"}
For Normal random variables and *continuous* random variables in general, the $\mathbb{P}(\cdot)$ notation is a "shorthand" for a definite integral of the random variable.
:::
::::
:::: {.column width="32.5%"}
::: {.definition style="width: 100%;"}
$$
\mathbb{P}(a < X < b) = \int^b_a \!\! f_X(x;\boldsymbol{\theta})\, dx
$$
:::
::::
:::::

:::: {.fragment fragment-index=0 style="width: 67.5%"}
This is straightforward to implement in [Python]{.cornflowerblue}.

```{python mole-calcs-1}
#| echo: true

# Load in two submodules from the scipy package, and the math package
from scipy import integrate
from scipy import stats
import math

# Calculate P(150 < X < 206)
integrate.quad(stats.norm.pdf, 150, 206, args = (150, 56))[0]
```

```{python mole-calcs-2}
#| echo: true
#| classes: fragment

# Calculate P(X < 100)
integrate.quad(stats.norm.pdf, -math.inf, 100, args = (150, 56))[0]
```

```{python mole-calcs-3}
#| echo: true
#| classes: fragment

# Calculate P(124.5 < X < 125.5)
integrate.quad(stats.norm.pdf, 124.5, 125.5, args = (150, 56))[0]
```
::::

## Calculation of Probabilities by hand

:::::: {.columns}
::::: {.column width="87.5%"}
We use **Table A** instead of directly integrating the Normal random variable, which presents $\mathbb{P}(0 < Z < z)$ to four decimal places, where $Z \sim \text{Normal}(\mu = 0, \sigma = 1)$^a^.

:::: {.fragment style="margin-top: 50px;"}
A formal method shows that the following statement is true when we express the *endpoints* of an [event]{.steelblue} in terms of [*z*]{.MathJax-Font}-scores^b^ for Normal random variables.

::: {.definition-orchid style="width: 50%;"}
$$
\mathbb{P}(a < X < b) = \mathbb{P}\!\left(\frac{a - \mu}{\sigma} < Z < \frac{b - \mu}{\sigma}\right)
$$

:::
::::

::: {.fragment style="margin-top: 50px;"}
Recall for [**Ex 2**]{.red} that we let $X \sim \text{Normal}(\mu = 150, \sigma = 56)$. Hence, calculate by hand:

- $\mathbb{P}(150 < X < 206)$ [&zigrarr; $\mathbb{P}(0 < Z < 1) \approx 0.3413$]{.printOnly}
- $\mathbb{P}(X < 100)$ [&zigrarr; $\mathbb{P}(Z < -0.89) = \ldots \approx 0.1867$]{.printOnly}
- $\mathbb{P}(124.5 < X < 125.5)$ [&zigrarr; $\mathbb{P}(-0.46 < Z < -0.44) = \ldots \approx 0.0072$]{.printOnly}
:::
:::::
::::: {.column width="12.5%"}
:::: {style="padding-left: 24pt;"}
::: {.definition style="width: 100%;"}
$$
z = \frac{x - \mu}{\sigma}
$$
:::
::::
:::::
::::::

::: {.aside}
a. This choice of parameter values for a Normal random variable is known as the Standard Normal random variable.
b. A *z*-score expresses an [outcome]{.tomato} of a Normal random variable in *standard deviation* units.
:::

## Percentiles with Normal Random Variables

:::::: {.columns}
::::: {.column width="87.5%"}
The [outcome]{.tomato} associated with a Normal random variable's *percentile*, $p$, can be determined with the following probability statement^[Where $Z \sim \text{Normal}(\mu = 0, \sigma = 1)$.].

:::: {.dummy}
::: {.definition-orchid style="width: 40%;"}
$$
\mathbb{P}(X < x) = \mathbb{P}\!\left(Z < \frac{x - \mu}{\sigma}\right) = p
$$
:::
::::

::: {.fragment style="margin-top: 50px;"}
Recall for [**Ex 2**]{.red} that we let $X \sim \text{Normal}(\mu = 150, \sigma = 56)$. Hence, determine the [outcome]{.tomato} associated with the 86th percentile. [&zigrarr; $\mathbb{P}(X < x) = 0.86 \Rightarrow x \approx 210.48$]{.printOnly}
:::

::: {.fragment style="margin-top: 50px;"}
[Python]{.cornflowerblue} can determine the "exact" [outcome]{.tomato} associated with the 86th percentile.

```{python mole-percentile}
#| echo: true

# Load in the stats submodule from the scipy package
from scipy import stats

# Determine the "exact" outcome associated with the 86th percentile
stats.norm.ppf(0.86, 150, 56)
```
:::
:::::
::::: {.column width="12.5%"}
:::: {style="padding-left: 24pt;"}
::: {.definition style="width: 100%;"}
$$
x = \mu + z \sigma
$$
:::
::::
:::::
::::::

## Empirical Rule for Normal Distributions

::::: {.columns}
:::: {.column}
You may have noticed that the Normal distribution is symmetrically distributed about its mean.

::: {.fragment fragment-index=0 style="margin-top: 50px;"}
This allows us to easily calculate the probability of observing an event centred on its mean in terms of standard deviations.
:::
::::
:::: {.fragment fragment-index=0 .column}
```{r ER-1}
#| fig-dpi: 300
#| out-width: 80%
#| fig-height: 3.5
#| fig-width: 6.5
#| fig-cap: <strong>Figure&colon;</strong> $\mathbb{P}(\mu - \sigma < X < \mu + \sigma) \approx 0.6827$.

ER <- \(z) {
  par(mai = c(0.6, 0.65, 0, 0))
  z0 <- seq(-4, 4, length.out = 1000)
  plot(0, type = "n", xlim = c(-3, 3), ylim = c(0, dnorm(0) * 1.1), axes = FALSE, yaxs = "i", xlab = "", ylab = "")
  lines(x = z0, y = dnorm(z0), col = "gray")
  abline(h = 0)
  axis(1, tcl = -0.2, mgp = c(0.75, 0.5, 0)); mtext("x", side = 1, line = 1.7, font = 2)
  axis(2, tcl = -0.2, mgp = c(0.75, 0.5, 0), las = 2); mtext("f(x; μ, σ)", side = 2, line = 2.25, font = 2)
  
  zz <- seq(-z, z, length.out = 1000)
  
  polygon(x = c(zz, rev(zz)), y = c(dnorm(zz), rep(0, length(zz))), col = "yellow", lwd = 2)
}

ER(1)
```
::::
:::: {.fragment fragment-index=1 .column}
```{r ER-2}
#| fig-dpi: 300
#| out-width: 80%
#| fig-height: 3.5
#| fig-width: 6.5
#| fig-cap: <strong>Figure&colon;</strong> $\mathbb{P}(\mu - 2\sigma < X < \mu + 2\sigma) \approx 0.9545$.

ER(2)
```
::::
:::: {.fragment fragment-index=2 .column}
```{r ER-3}
#| fig-dpi: 300
#| out-width: 80%
#| fig-height: 3.5
#| fig-width: 6.5
#| fig-cap: <strong>Figure&colon;</strong> $\mathbb{P}(\mu - 3\sigma < X < \mu + 3\sigma) \approx 0.9973$.

ER(3)
```
::::
:::::

## [Ex 3:]{.red}

## Data in terms of Random Variables

## Normally Distributed Data

## Sample Statistics as Random Variables

## Central Limit Theorem

Properties of Random Variables

## Central Limit Theorem in Action

## [Ex 4:]{.red}

## [Glossary]{.darkorange}

## Practice Exercises {visibility="uncounted"}

<ol style="line-height: 1.1; font-size: 0.8em;">
  <li style="margin-bottom: 50pt;">
  The diameter of individual rolling pins produced in a factory can be modelled by a Normal random variable, with a mean of 22.5 mm and a standard deviation of 0.5 mm.
    <ol>
      <li style="list-style-type: lower-alpha; line-height: 1.1;">
      Calculate the probability that the next rolling pin produced has a diameter smaller than 23.26 mm.
      </li>
      <li style="list-style-type: lower-alpha; line-height: 1.1;">
      Calculate the probability that the next rolling pin produced has a diameter greater than 23.26 mm.
      </li>
      <li style="list-style-type: lower-alpha; line-height: 1.1;">
      Suppose that 5000 rolling pins were produced. Pins acceptable for sale must have a diameter between 21.5 and 23.5 mm. How many pins are likely to be acceptable for sale (according to a Normal distribution)?
      </li>
      <li style="list-style-type: lower-alpha; line-height: 1.1;">
      The largest 2% of rolling pins need to be reworked to reduce their size. What is the value of the diameter that just 2% of the rolling pins produced would exceed (according to a Normal distribution)?
      </li>
    </ol>
  </li>
  
  <li>
  Suppose beer is sold in 330 ml bottles, and each bottle is supposed to contain 330 ml of beer. Due to variations in the manufacturing process, the real amount of beer is approximately Normally distributed with a mean of 328 ml and a standard deviation of 3 ml.
    <ol>
      <li style="list-style-type: lower-alpha; line-height: 1.1;">
      Calculate the probability that a randomly selected bottle contains less than 330 ml.
      </li>
      <li style="list-style-type: lower-alpha; line-height: 1.1;">
      Calculate the probability that the *average* volume of bottles of beer in a randomly selected crate of 24 bottles contains more than 330 ml.
      </li>
      <li style="list-style-type: lower-alpha; line-height: 1.1;">
      Explain which theorem you used in part b. to calculate the requested probability and whether the conditions to use the theorem were met.
      </li>
    </ol>
  </li>
</ol>
