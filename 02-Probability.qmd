---
title: "Probability"
subtitle: "_**with a focus on the Normal Distribution and Central Limit Theorem**_"
author: "ENGEN102-23G (HAM) - Engineering Maths and Modelling 1B"
format:
  revealjs: 
    slide-number: c/t
    progress: true
    auto-stretch: false
    height: 1080
    width: 1920
    theme: reveal-extra/quartz.scss
    template-partials:
      - reveal-extra/title-block.html
    highlight-style: breeze
    code-line-numbers: false
    embed-resources: true
editor: source
---

```{r setup}
#| include: false
library(knitr)
library(ggplot2)
```

## Outcomes[, events]{.fragment fragment-index=1}[, and probabilities]{.fragment fragment-index=2}

::::: {.columns}
:::: {.column width="65%"}
An [outcome]{.tomato} is a *possible* value we can observe.

<blockquote>
The outcomes of rolling a fair six-sided dice are 1, 2, 3, 4, 5, and 6.
</blockquote>

::: {.fragment fragment-index=1 style="margin-top: 50px;"}
An [event]{.steelblue} is a *set of distinct* [outcome(s)]{.tomato} we can observe.

<blockquote style="border-left: 0.25em solid steelblue;">
The event that I rolled an odd number with a fair six-sided dice consists of the outcomes 1, 3, and 5.
</blockquote>
:::

::: {.fragment fragment-index=2 style="margin-top: 50px; padding-right: 4pt;"}
A [probability]{.orchid} is a *real number*, $\mathbb{R}$, that takes a value between 0 and 1 **inclusive**, which describes the occurrence of an [event]{.steelblue} based on the "long-run" frequency of the [event]{.steelblue}.

<blockquote style="border-left: 0.25em solid orchid;">
The probability of the event that I rolled an odd number with a fair six-sided dice is 0.5.
</blockquote>
:::
::::
:::: {.column width="35%"}
![**Figure:** Two six-sided dice by [glitch](https://openclipart.org/detail/210239/misc-dice) (CC0 1.0).](figures/misc-dice.svg)
::::
:::::

## Random Variables

In Statistics, a **random variable** summarises the [outcomes]{.tomato} (represented as numeric values) of a process by [_**chance alone**_]{.fragment .highlight-blue}.

::: {.fragment style="margin-top: 50px;"}
It is a function whose *domain* is all possible distinct [outcomes]{.tomato} we can observe from a process, and each [outcome]{.tomato} is mapped to a *non-negative* real number, $\mathbb{R}_{\geq0}$, according to the probability axioms^[See the [Wikipedia](https://en.wikipedia.org/wiki/Probability_axioms) page on the probability axioms you are interested in what they are.].
:::

::: {.fragment style="margin-top: 50px;"}
This function is known as the **probability distribution** (that the random variable follows). [As a result, it is standard to denote a random variable with uppercase letters [*X*]{.MathJax-Font}, [*Y*]{.MathJax-Font}, etc. and its [outcomes]{.tomato} with lowercase letters [*x*]{.MathJax-Font}, [*y*]{.MathJax-Font}, etc.]{.fragment}
:::

## More on Random Variables

A **random variable** can be used to model the [outcomes]{.tomato} we could observe from a process by _**chance alone**_. [But why do we assign a *non-negative* real number to [outcomes]{.tomato}?]{.fragment}

:::::: {.fragment style="margin-top: 50px;"}
::::: {.columns}
:::: {.column width="55%"}
Consider the following cases:

::: {.li-line-height-1-1 style="padding-right: 24pt;"}
1. A fair six-sided die, whose [outcomes]{.tomato} are 1, 2, 3, 4, 5, and 6.
2. A uniform random number generator, whose [outcomes]{.tomato} are any real number between 1<br>and 6 inclusive.
:::
::::
:::: {.column width="45%"}
```{python random-numbers}
#| echo: true
#| class: fragment

# Load in the random package
import random

# One "simulated" die roll
print(random.randint(1, 6))

# One uniform random number
print(random.uniform(1, 6))
```
::::
:::::
::::::

::: {.fragment style="margin-top: 50px;"}
*Case 1* is an example of a *discrete* random variable, whereas *Case 2* is an example of a *continuous* random variable. 
:::

::: {.fragment style="margin-top: 50px;"}
The key takeaway is that we should _**always**_ assign [probabilities]{.orchid} to [events]{.steelblue}, *regardless* of the type of random variable.
:::

## Normal Distribution

Let $X$ be a *continuous* random variable that follows the *Normal distribution*.

::: {.fragment .definition style="width: 25%; margin-top: 50px;"}
$$
X \sim \text{Normal}(\mu, \sigma)
$$
:::

:::: {.fragment style="margin-top: 50px;"}
The **probability distribution** of a Normally distributed random variable is determined by

::: {.definition-orchid style="width: 55%; display: block; margin: auto;"}
$$
f_X(x; \mu, \sigma) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-(x-\mu)^2/2\sigma^2}, ~ \forall ~ x \in \mathbb{R}.
$$
:::

::: {style="margin-top: 25px;"}
where its *parameters* are the mean of the Normal distribution, [$\mu$]{.blue}, and the standard deviation of the Normal distribution, [$\sigma$]{.blue}, provided that $\mu \in \mathbb{R}$ and $\sigma \in \mathbb{R}_{>0}$.
:::
::::

::: {.aside}
**Note:** The *Normal distribution* is also known as the *Gaussian distribution*
:::

## [Ex 2:]{.red} Unwanted Moles

::::: {.columns}
:::: {.column width="70%"}
<blockquote style="border-left: 0.25em solid #685f5f;">
Stewart (1994) reported on data collected by a company that removed unwanted moles. It was found that the weights of the moles were approximately Normally distributed, with a mean of 150 grams and a standard deviation of 56 grams.
</blockquote>

::: {.fragment style="margin-top: 50px;"}
What process could describe how we can observe the weights of moles by _**chance alone**_? [Ideally, a random sample.]{.printOnly}
:::

::::
:::: {.column width="30%"}
![**Figure:** Mole by Amethyst Studio from<br>[**Figure:**]{style="visibility: hidden;"} [Noun Project](https://thenounproject.com/browse/icons/term/mole/) (CC BY 3.0).](figures/noun-mole.svg)
::::
:::::

::: {.aside}
Stewart, Ian (17 September 1994). Statistical modelling. *New Scientist: Inside Science*, **74**, 14.
:::

## Events with Normal Random Variables

:::::: {.columns}
::::: {.column  width="70%"}
For [**Ex 2**]{.red}, we let $X \sim \text{Normal}(\mu = 150, \sigma = 56)$ and note that the units are in grams.

:::: {.fragment style="margin-top: 50px;" fragment-index=0}
An [event]{.steelblue} for *any* Normal random variable involves an interval and, in rare cases, *disjoint* intervals. [For example:]{.fragment fragment-index=1}

::: {.fragment fragment-index=1}
- A mole has a weight that is between 150 to 206 grams.
- A mole has a weight that is lower than 100 grams.
- A mole has a weight that is 125 &pm; 0.5 grams.
:::
::::

:::::
::::: {.column width="30%"}
```{r mole-norm}
#| fig-dpi: 300
#| out-width: 100%
#| fig-align: center
#| fig-width: 8
#| fig-height: 7
#| fig-cap: "**Figure:** A Normal random variable with<br>[**Figure:**]{style=\"visibility: hidden\"} [*&mu;* = 150]{.MathJax-Font} and [*&sigma;* = 56]{.MathJax-Font}."

par(mai = c(0.6, 0.65, 0, 0))
curve(dnorm(x, 150, 56), type = "n", xlim = 150 + c(-3, 3) * 56, ylim = c(0, dnorm(150, 150, 56) * 1.1), axes = FALSE, yaxs = "i", xlab = "", ylab = "")
abline(h = 0)
axis(1, tcl = -0.2, mgp = c(0.75, 0.5, 0))
mtext("x", side = 1, line = 1.7, font = 2)
axis(2, tcl = -0.2, mgp = c(0.75, 0.5, 0), las = 2, cex.axis = 0.75, at = c(seq(0, 0.007, length.out = 8), 1))
mtext("f(x; 150, 56)", side = 2, line = 2.25, font = 2)
curve(dnorm(x, 150, 56), lwd = 2, add = TRUE, from = 150 - 4 * 56, to = 150 + 4 * 56, col = "darkblue", n = 501)
```
:::::
::::::

## Probability Statements with Normal Random Variables

::::: {.columns}
:::: {.column  width="70%"}
In Statistics, we use the $\mathbb{P}(\cdot)$ notation to denote the [probability]{.orchid} of an [event]{.steelblue}.

Let's assume for [**Ex 2**]{.red} that a random sample is the process observing the weights of moles:

::: {.incremental .li-line-height-1-1}
- The [probability]{.orchid} of a randomly selected mole weighs somewhere between 150 to 206 grams. [&zigrarr; $\mathbb{P}(150 < X < 206)$]{.printOnly}
- The [probability]{.orchid} of a randomly selected mole has a weight lower than 100 grams. [&zigrarr; $\mathbb{P}(X < 100)$]{.printOnly}
- The [probability]{.orchid} of a randomly selected mole weighs 125 &pm; 0.5 grams. [&zigrarr; $\mathbb{P}(124.5 < X < 125.5)$]{.printOnly}

:::
::::
:::: {.column  width="30%"}
```{r mole-norm}
#| fig-dpi: 300
#| out-width: 100%
#| fig-align: center
#| fig-width: 8
#| fig-height: 7
#| fig-cap: "**Figure:** A Normal random variable with<br>[**Figure:**]{style=\"visibility: hidden\"} [*&mu;* = 150]{.MathJax-Font} and [*&sigma;* = 56]{.MathJax-Font}."
```
::::
:::::

## Calculation of Probabilities in [Python]{.cornflowerblue .fragment fragment-index=0}

::::: {.columns}
:::: {.column width="67.5%"}
::: {style="padding-right: 24pt;"}
For Normal random variables and *continuous* random variables in general, the $\mathbb{P}(\cdot)$ notation is a "shorthand" for a definite integral of the random variable.
:::
::::
:::: {.column width="32.5%"}
::: {.definition style="width: 100%;"}
$$
\mathbb{P}(a < X < b) = \int^b_a \!\! f_X(x;\boldsymbol{\theta})\, dx
$$
:::
::::
:::::

:::: {.fragment fragment-index=0 style="width: 67.5%"}
This is straightforward to implement in [Python]{.cornflowerblue}.

```{python mole-calcs-1}
#| echo: true

# Load in two submodules from the scipy package, and the math package
from scipy import integrate
from scipy import stats
import math

# Calculate P(150 < X < 206)
integrate.quad(stats.norm.pdf, 150, 206, args = (150, 56))[0]
```

```{python mole-calcs-2}
#| echo: true
#| classes: fragment

# Calculate P(X < 100)
integrate.quad(stats.norm.pdf, -math.inf, 100, args = (150, 56))[0]
```

```{python mole-calcs-3}
#| echo: true
#| classes: fragment

# Calculate P(124.5 < X < 125.5)
integrate.quad(stats.norm.pdf, 124.5, 125.5, args = (150, 56))[0]
```
::::

## Calculation of Probabilities by hand

:::::: {.columns}
::::: {.column width="87.5%"}
We use **Table A** instead of directly integrating the Normal random variable, which presents $\mathbb{P}(0 < Z < z)$ to four decimal places, where $Z \sim \text{Normal}(\mu = 0, \sigma = 1)$^a^.

:::: {.fragment style="margin-top: 50px;"}
A formal method shows that the following statement is true when we express the *endpoints* of an [event]{.steelblue} in terms of [*z*]{.MathJax-Font}-scores^b^ for Normal random variables.

::: {.definition-orchid style="width: 50%; margin-top: 50px;"}
$$
\mathbb{P}(a < X < b) = \mathbb{P}\!\left(\frac{a - \mu}{\sigma} < Z < \frac{b - \mu}{\sigma}\right)
$$

:::
::::

::: {.fragment style="margin-top: 50px;"}
Recall for [**Ex 2**]{.red} that we let $X \sim \text{Normal}(\mu = 150, \sigma = 56)$. Hence, calculate by hand:

- $\mathbb{P}(150 < X < 206)$ [&zigrarr; $\mathbb{P}(0 < Z < 1) \approx 0.3413$]{.printOnly}
- $\mathbb{P}(X < 100)$ [&zigrarr; $\mathbb{P}(Z < -0.89) = \ldots \approx 0.1867$]{.printOnly}
- $\mathbb{P}(124.5 < X < 125.5)$ [&zigrarr; $\mathbb{P}(-0.46 < Z < -0.44) = \ldots \approx 0.0072$]{.printOnly}
:::
:::::
::::: {.column width="12.5%"}
:::: {style="padding-left: 24pt;"}
::: {.definition style="width: 100%;"}
$$
z = \frac{x - \mu}{\sigma}
$$
:::
::::
:::::
::::::

::: {.aside}
a. This choice of parameter values for a Normal random variable is known as the Standard Normal random variable.
b. A *z*-score expresses an [outcome]{.tomato} of a Normal random variable in *standard deviation* units.
:::

## Percentiles with Normal Random Variables

:::::: {.columns}
::::: {.column width="87.5%"}
The [outcome]{.tomato} associated with a Normal random variable's *percentile*, $p$, can be determined with the following probability statement^[Where $Z \sim \text{Normal}(\mu = 0, \sigma = 1)$.].

:::: {.dummy}
::: {.definition-orchid style="width: 40%; margin-top: 50px;"}
$$
\mathbb{P}(X < x) = \mathbb{P}\!\left(Z < \frac{x - \mu}{\sigma}\right) = p
$$
:::
::::

::: {.fragment style="margin-top: 50px;"}
Recall for [**Ex 2**]{.red} that we let $X \sim \text{Normal}(\mu = 150, \sigma = 56)$. Hence, determine the [outcome]{.tomato} associated with the 86th percentile. [&zigrarr; $\mathbb{P}(X < x) = 0.86 \Rightarrow x \approx 210.48$]{.printOnly}
:::

::: {.fragment style="margin-top: 50px;"}
[Python]{.cornflowerblue} can determine the "exact" [outcome]{.tomato} associated with the 86th percentile.

```{python mole-percentile}
#| echo: true

# Load in the stats submodule from the scipy package
from scipy import stats

# Determine the "exact" outcome associated with the 86th percentile
stats.norm.ppf(0.86, 150, 56)
```
:::
:::::
::::: {.column width="12.5%"}
:::: {style="padding-left: 24pt;"}
::: {.definition style="width: 100%;"}
$$
x = \mu + z \sigma
$$
:::
::::
:::::
::::::

## Empirical Rule for Normal Distributions

You may have noticed that the Normal distribution is *symmetrically* distributed. This fact allows us to calculate the [probability]{.orchid} of an [event]{.steelblue} centred on the mean.

:::: {.columns}
::: {.fragment .column width="33.33%"}
```{r ER-1}
#| fig-dpi: 300
#| out-width: 100%
#| fig-width: 8
#| fig-height: 7
#| fig-cap: <strong>Figure&colon;</strong> $\mathbb{P}(\mu - \sigma < X < \mu + \sigma) \approx 0.6827$.

ER <- \(z) {
  par(mai = c(0.6, 0.65, 0, 0))
  z0 <- seq(-4, 4, length.out = 1000)
  plot(0, type = "n", xlim = c(-3, 3), ylim = c(0, dnorm(0) * 1.1), axes = FALSE, yaxs = "i", xlab = "", ylab = "")
  lines(x = z0, y = dnorm(z0), col = "gray")
  abline(h = 0)
  axis(1, tcl = -0.2, mgp = c(0.75, 0.5, 0)); mtext("x", side = 1, line = 1.7, font = 2)
  axis(2, tcl = -0.2, mgp = c(0.75, 0.5, 0), las = 2, at = c(0, 0.1, 0.2, 0.3, 0.4, 1.0)); mtext("f(x; μ, σ)", side = 2, line = 2.25, font = 2)
  
  zz <- seq(-z, z, length.out = 1000)
  
  polygon(x = c(zz, rev(zz)), y = c(dnorm(zz), rep(0, length(zz))), col = "yellow", lwd = 2)
}

ER(1)
```
:::
::: {.fragment width="33.33%" .column}
```{r ER-2}
#| fig-dpi: 300
#| out-width: 100%
#| fig-width: 8
#| fig-height: 7
#| fig-cap: <strong>Figure&colon;</strong> $\mathbb{P}(\mu - 2\sigma < X < \mu + 2\sigma) \approx 0.9545$.

ER(2)
```
:::
::: {.fragment width="33.33%" .column}
```{r ER-3}
#| fig-dpi: 300
#| out-width: 100%
#| fig-width: 8
#| fig-height: 7
#| fig-cap: <strong>Figure&colon;</strong> $\mathbb{P}(\mu - 3\sigma < X < \mu + 3\sigma) \approx 0.9973$.

ER(3)
```
:::
::::

## [Ex 3:]{.red} Dielectric Breakdown Voltages

::::: {.columns}
:::: {.column width="70%"}
<blockquote style="border-left: 0.25em solid #685f5f;">
The `breakdown.csv` file describes a subset of data about the dielectric breakdown voltage of 20 epoxy resin test pieces in an experimental setting. A dielectric breakdown occurs when a material suddenly becomes a conductor.
</blockquote>

::: {.fragment fragment-index=1 .orchid-table style="font-size: 0.9em; margin-top: 50px; width: 90%;"}
+:------------+:-----------------------------------------------------------------------------------------------+
| **Item**    | A number that denotes the identifier of a test piece.
+-------------+------------------------------------------------------------------------------------------------+
| **Case**    | A number that denotes which experimental setting was tested.
+-------------+------------------------------------------------------------------------------------------------+
| **Voltage** | A number that denotes the dielectric breakdown voltage of a test piece, measured in volts (V).
+-------------+------------------------------------------------------------------------------------------------+
:::
::::
:::: {.column width="30%" .fragment fragment-index=0}
```{r alloy} 
#| results: asis
#| classes: excel

breakdown.df <- read.csv("datasets/breakdown.csv")
 
rbind(colnames(breakdown.df), head(breakdown.df, n = 13), rep("...", ncol(breakdown.df))) |>
  kable(format = "html", row.names = TRUE, col.names = LETTERS[1:ncol(breakdown.df)])
```
::::
:::::

::: {.aside}
Data from Hirose, H. (1996). Maximum Likelihood Estimation in the 3-parameter Weibull Distribution. A look through the generalized extreme-value distribution. *IEEE Transactions on Dielectrics and Electrical Insulation*, **3**(1), 43--55.
:::

## Data in terms of Random Variables

::::: {.columns}
:::: {.column width="65%"}
Data can be described as the realisations of $n$ identical **random variables**, $X_1, X_2, \ldots, X_n$.

::: {.fragment fragment-index=1 style="margin-top: 50px;"}
When we describe data as being (approximately) Normally distributed, we imply that the $n$ identical **random variables** follow the Normal distribution.
:::

::: {.fragment fragment-index=2 style="margin-top: 50px;"}
If we _**assume**_ independence^[That is, the [outcome]{.tomato} of each **random variable** has no influence on the [outcomes]{.tomato} of all other **random variables**.], the sample mean and sample standard deviation, $\bar{x}$ and $s$, are the best estimates of the Normal distribution's $\mu$ and $\sigma$ parameters, respectively.
:::

::::
:::: {.column .fragment fragment-index=0 width="35%"}
```{r Ex4}
#| fig-dpi: 300
#| fig-height: 4
#| fig-width: 7
#| out-width: 100%
#| fig-cap: "**Figure:** Distribution of 20 epoxy resin test [**Figure:**]{style=\"visibility: hidden;\"} pieces' dielectric voltages."
ggplot(breakdown.df, aes(x = Voltage)) +
  labs(x = "Dielectric voltage (V)", title = " Distribution of 20 epoxy resin test pieces") +
  theme_bw() + 
  geom_histogram(center = 0, binwidth = (max(breakdown.df$Voltage) - min(breakdown.df$Voltage)) / 10, colour = "black", fill = "lightblue") + 
  labs(y = NULL) + 
  scale_y_continuous(breaks = NULL)
```
::::
:::::

## Sample Statistics as Random Variables

A convenient consequence of the previous slide is that numeric summaries, such as the sample mean and standard deviation, can also be considered **random variables**.

::::: {.columns}
:::: {.column .fragment}
::: {.definition style="width: 40%; margin-left: auto !important; margin-right: auto !important; margin-top: 50px;"}
$$
\bar{X} = \frac{\sum^n_{i=1} X_i}{n}
$$
:::
::::
:::: {.column .fragment}
::: {.definition-orchid style="width: 60%; margin-left: auto !important; margin-right: auto !important; margin-top: 50px;"}
$$
S = \sqrt{\frac{\sum^n_{i=1} (X_i)^2 - n\bar{X}^2}{n - 1}}
$$
:::
::::
:::::

::: {.fragment style="margin-top: 50px;"}
But which probability distribution do these **random variables** follow?
:::

## Central Limit Theorem

Let $X_1, X_2, \cdots, X_n$ be $n$ independent and identically distributed **random variables** with *finite* mean, [$\mu_X$]{.blue}, and standard deviation, [$\sigma_X$]{.blue}.

:::: {.fragment fragment-index=0 style="margin-top: 50px;"}
Let the **random variable** $S_n = X_1 + X_2 + \cdots + X_n$[, then *Central Limit Theorem* states that]{.fragment fragment-index=1}

::: {.definition style="width: 50%; margin-top: 50px;" .fragment fragment-index=1}
$$
S_n \rightarrow_D \text{Normal} \! \left(\mu = n\mu_X, \sigma = \sqrt{n}\,\sigma_X\right).
$$
:::
::::

:::: {.incremental .fragment style="margin-top: 50px;"}
How large does $n$ has to be to ensure the convergence is good enough to use a Normally distributed **random variable** to model $S_n$?

::: {style="margin-top: -25px;"}
- $n \geq 30$ for the majority of **probability distributions** with finite [$\mu_X$]{.blue} and [$\sigma_X$]{.blue}.
:::
::::

::: {.aside}
$\rightarrow_D$ means converges in distribution.
:::

## Central Limit Theorem in Action

:::::: {.panel-tabset}

### "Bathtub"

::::: {.columns}
:::: {.column}
```{r Bathtub-CLT}
#| fig-dpi: 300
#| out-width: 100%
#| fig-width: 6
#| fig-height: 5

ggplot() +
  stat_function(fun = dbeta, xlim = c(-1, 2), geom = "polygon", fill = "darkblue", alpha = 0.2, args = list(shape1 = 0.6, shape2 = 0.6), n = 5000) +
  geom_function(fun = dbeta, xlim = c(-1, 2), linewidth = 1, colour = "darkblue", args = list(shape1 = 0.6, shape2 = 0.6), n = 5000) +
  theme_bw() +
  labs(x = "x", y = "f(x; θ)") + 
  coord_cartesian(xlim = c(0, 1))

plotr <- \(nval) {
  ggplot(data = subset(test.data, n == nval), aes(x = x)) + 
    geom_histogram(aes(y = after_stat(density)), position = "identity", colour = "tomato", fill = rgb(255, 99, 71, 50, maxColorValue = 255), breaks = hist(subset(test.data, n == nval)$x, plot = FALSE)$breaks) +
    theme_bw() +
    labs(x = bquote("s"[.(nval)]), y = "Density")
}

n <- c(2, 3, 10, 50)
reps <- 10000
test.data <- data.frame(x = numeric(), n = numeric())

for (i in n) {
  test.data <- rbind(
    test.data,
    data.frame(x = matrix(data = rbeta(reps * i, shape1 = 0.6, shape2 = 0.6), nrow = reps, ncol = i) |>
                 rowSums(),
               n = i))
}
```
::::
:::: {.column}
::: {.panel-tabset}
#### n = `r n[1]`
```{r Bathtub-CLT-2A}
#| fig-dpi: 300
#| fig-width: 6
#| fig-height: 4.45

plotr(n[1])
```

#### n = `r n[2]`
```{r Bathtub-CLT-2B}
#| fig-dpi: 300
#| fig-width: 6
#| fig-height: 4.45

plotr(n[2])
```

#### n = `r n[3]`
```{r Bathtub-CLT-2C}
#| fig-dpi: 300
#| fig-width: 6
#| fig-height: 4.45

plotr(n[3])
```

#### n = `r n[4]`
```{r Bathtub-CLT-2D}
#| fig-dpi: 300
#| fig-width: 6
#| fig-height: 4.45

plotr(n[4])
```
:::
::::
:::::

### Normal

::::: {.columns}
:::: {.column}
```{r Normal-CLT}
#| fig-dpi: 300
#| out-width: 100%
#| fig-width: 6
#| fig-height: 5

ggplot() +
  stat_function(fun = dnorm, xlim = c(-4, 4), geom = "polygon", fill = "darkblue", alpha = 0.2) +
  geom_function(fun = dnorm, xlim = c(-4, 4), linewidth = 1, colour = "darkblue") +
  theme_bw() +
  labs(x = "x", y = "f(x; θ)") + 
  coord_cartesian(xlim = c(-3, 3))

n <- c(2, 3, 10, 50)
reps <- 10000
test.data <- data.frame(x = numeric(), n = numeric())

for (i in n) {
  test.data <- rbind(
    test.data,
    data.frame(x = matrix(data = rnorm(reps * i, mean = 0, sd = 1), nrow = reps, ncol = i) |>
                 rowSums(),
               n = i))
}
```
::::
:::: {.column}
::: {.panel-tabset}
#### n = `r n[1]`
```{r Normal-CLT-2A}
#| fig-dpi: 300
#| fig-width: 6
#| fig-height: 4.45

plotr(n[1])
```

#### n = `r n[2]`
```{r Normal-CLT-2B}
#| fig-dpi: 300
#| fig-width: 6
#| fig-height: 4.45

plotr(n[2])
```

#### n = `r n[3]`
```{r Normal-CLT-2C}
#| fig-dpi: 300
#| fig-width: 6
#| fig-height: 4.45

plotr(n[3])
```

#### n = `r n[4]`
```{r Normal-CLT-2D}
#| fig-dpi: 300
#| fig-width: 6
#| fig-height: 4.45

plotr(n[4])
```
:::
::::
:::::

### Uniform

::::: {.columns}
:::: {.column}
```{r Uniform-CLT}
#| fig-dpi: 300
#| out-width: 100%
#| fig-width: 6
#| fig-height: 5

ggplot() +
  stat_function(fun = dunif, xlim = c(-2, 3), geom = "polygon", fill = "darkblue", alpha = 0.2, n = 5000) +
  geom_function(fun = dunif, xlim = c(-4, 4), linewidth = 1, colour = "darkblue", n = 5000) +
  theme_bw() +
  labs(x = "x", y = "f(x; θ)") + 
  coord_cartesian(xlim = c(-1, 2))

n <- c(2, 3, 10, 50)
reps <- 10000
test.data <- data.frame(x = numeric(), n = numeric())

for (i in n) {
  test.data <- rbind(
    test.data,
    data.frame(x = matrix(data = runif(reps * i, min = 0, max = 1), nrow = reps, ncol = i) |>
                 rowSums(),
               n = i))
}
```
::::
:::: {.column}
::: {.panel-tabset}
#### n = `r n[1]`
```{r Uniform-CLT-2A}
#| fig-dpi: 300
#| fig-width: 6
#| fig-height: 4.45

plotr(n[1])
```

#### n = `r n[2]`
```{r Uniform-CLT-2B}
#| fig-dpi: 300
#| fig-width: 6
#| fig-height: 4.45

plotr(n[2])
```

#### n = `r n[3]`
```{r Uniform-CLT-2C}
#| fig-dpi: 300
#| fig-width: 6
#| fig-height: 4.45

plotr(n[3])
```

#### n = `r n[4]`
```{r Uniform-CLT-2D}
#| fig-dpi: 300
#| fig-width: 6
#| fig-height: 4.45

plotr(n[4])
```
:::
::::
:::::

### Six-Sided Dice

::::: {.columns}
:::: {.column}
```{r Dice-CLT}
#| fig-dpi: 300
#| out-width: 100%
#| fig-width: 6
#| fig-height: 5

ggplot() +
  geom_segment(aes(x = 1:6, xend = 1:6, y = 0, yend = 1/6), linewidth = 1, colour = "darkblue") +
  geom_hline(aes(yintercept = 0), linewidth = 1, colour = "darkblue") +
  geom_point(aes(x = 1:6, y = 1/6), size = 4, colour = "darkblue") +
  theme_bw() +
  labs(x = "x", y = "f(x; θ)") + 
  coord_cartesian(xlim = c(0, 7))

n <- c(2, 3, 10, 50)
reps <- 10000
test.data <- data.frame(x = numeric(), n = numeric())

for (i in n) {
  test.data <- rbind(
    test.data,
    data.frame(x = matrix(data = sample(1:6, size = reps * i, replace = TRUE), nrow = reps, ncol = i) |>
                 rowSums(),
               n = i))
}
```
::::
:::: {.column}
::: {.panel-tabset}
#### n = `r n[1]`
```{r Dice-CLT-2A}
#| fig-dpi: 300
#| fig-width: 6
#| fig-height: 4.45

plotr(n[1])
```

#### n = `r n[2]`
```{r Dice-CLT-2B}
#| fig-dpi: 300
#| fig-width: 6
#| fig-height: 4.45

plotr(n[2])
```

#### n = `r n[3]`
```{r Dice-CLT-2C}
#| fig-dpi: 300
#| fig-width: 6
#| fig-height: 4.45

plotr(n[3])
```

#### n = `r n[4]`
```{r Dice-CLT-2D}
#| fig-dpi: 300
#| fig-width: 6
#| fig-height: 4.45

plotr(n[4])
```
:::
::::
:::::
::::::


## Sampling Distribution of the Sample Mean

One outcome of the *Central Limit Theorem* is the **sampling distribution of the sample mean**.

. . .

::: {.definition style="margin-top: 50px; width: 90%;"}
Let $X_1, X_2, \cdots, X_n$ be $n$ independent and identically distributed **random variables** with *finite* mean, [$\mu_X$]{.blue}, and standard deviation, [$\sigma_X$]{.blue}. If we let $\bar{X} = \frac{\sum^n_{i=1} X_i}{n}$, then

$$
\bar{X} \rightarrow_D \text{Normal} \! \left(\mu = \mu_X, \sigma = \frac{\sigma_X}{\sqrt{n}}\right)
$$
:::

. . .

::: {style="margin-top: 50px;"}
Similarly, $n \geq 30$ is "good enough" to use a Normally distributed **random variable** to model $\bar{X}$. Furthermore, if $X_i \sim \text{Normal}(\mu_X, \sigma_X)$, then $\bar{X} \sim \text{Normal} \! \left(\mu = \mu_X, \sigma = \frac{\sigma_X}{\sqrt{n}}\right)$.
:::

## [Ex 2:]{.red} Unwanted Moles [(revisited)]{style="font-size: 0.7em;"}

::::: {.columns}
:::: {.column width="70%"}
<blockquote style="border-left: 0.25em solid #685f5f;">
Stewart (1994) reported on data collected by a company that removed unwanted moles. It was found that the weights of the moles were approximately Normally distributed, with a mean of 150 grams and a standard deviation of 56 grams.
</blockquote>

::: {.incremental style="margin-top: 50px;"}
Suppose we are modelling the mean weight for a random sample of 16 moles. 

<ol>
  <li style="line-height: 1.1;"> What is the [probability]{.orchid} of observing a sample mean weight greater than 200 grams? [&zigrarr; 0.0002]{.printOnly}</li>
  <li style="line-height: 1.1;"> What is the 50th percentile of the sample mean weight according to an *appropriate* probability distribution? [&zigrarr; 150 g]{.printOnly}</li>
  <li style="line-height: 1.1;"> What is the interval associated with the [probability]{.orchid} of observing a sample mean weight that is within one standard deviation of the mean? [&zigrarr; [136 g, 164 g]]{.printOnly}</li>
</ol>
:::

::::
:::: {.column width="30%"}
![**Figure:** Mole by Amethyst Studio from<br>[**Figure:**]{style="visibility: hidden;"} [Noun Project](https://thenounproject.com/browse/icons/term/mole/) (CC BY 3.0).](figures/noun-mole.svg)
::::
:::::

::: {.aside}
Stewart, Ian (17 September 1994). Statistical modelling. *New Scientist: Inside Science*, **74**, 14.
:::




## [Glossary]{.darkorange}

:::: {style="font-size: 0.8em;"}
Let $X$ be a *continuous* random variable that follows the Normal distribution with mean $\mu$ and standard deviation $\sigma$.

::: {.parskip-16pt style="margin-top: -0.5em;"}
- [$\mathbb{P}(a < X < b) > 0$]{.blue} when $b > a$.
- [$\mathbb{P}(X = x) = 0$]{.blue} for *continuous* random variables because $\int_x^x \! f_X(x; \boldsymbol{\theta}) dx = 0$.
- [$\mathbb{P}(a < X < b) = \mathbb{P}(a \leq X < b) = \mathbb{P}(a < X \leq b) = \mathbb{P}(a \leq X \leq b)$]{.blue} as a consequence of $\mathbb{P}(X = x) = 0$.
- [$\mathbb{P}(X < x) = 1 - \mathbb{P}(X > x)$]{.blue} expresses the probability of an [event]{.orchid} in terms of the probability of its [complement event]{.teal}.
- [$\mathbb{P}(a < X < b) + \mathbb{P}(c < X < d)$]{.blue} is valid _**if and only if**_ the intervals $(a, b)$ and $(c, d)$ do *not* overlap.
:::

For the Normal distribution in particular:

::: {.parskip-16pt style="margin-top: -0.5em;"}
- [$\mathbb{P}(-|b| < X < -|a|) = \mathbb{P} (a < X < b)$]{.blue} as the Normal distribution is *symmetrically* distributed.
- [$\mathbb{P}(X < \mu) = \mathbb{P}(X > \mu) = 0.5$]{.blue} as the 50th percentile (the median) of the Normal distribution is its mean.
:::

::::

## Practice Exercises {visibility="uncounted"}

<ol style="line-height: 1.1; font-size: 0.8em;">
  <li style="margin-bottom: 50pt;">
  The diameter of individual rolling pins produced in a factory can be modelled by a Normal random variable, with a mean of 22.5 mm and a standard deviation of 0.5 mm.
    <ol>
      <li style="list-style-type: lower-alpha; line-height: 1.1;">
      Calculate the probability that the next rolling pin produced has a diameter smaller than 23.26 mm.
      </li>
      <li style="list-style-type: lower-alpha; line-height: 1.1;">
      Calculate the probability that the next rolling pin produced has a diameter greater than 23.26 mm.
      </li>
      <li style="list-style-type: lower-alpha; line-height: 1.1;">
      Suppose that 5000 rolling pins were produced. Pins acceptable for sale must have a diameter between 21.5 and 23.5 mm. How many pins are likely to be acceptable for sale (according to a Normal distribution)?
      </li>
      <li style="list-style-type: lower-alpha; line-height: 1.1;">
      The largest 2% of rolling pins need to be reworked to reduce their size. What is the value of the diameter that just 2% of the rolling pins produced would exceed (according to a Normal distribution)?
      </li>
    </ol>
  </li>
  
  <li>
  Suppose beer is sold in 330 ml bottles, and each bottle is supposed to contain 330 ml of beer. Due to variations in the manufacturing process, the real amount of beer is approximately Normally distributed with a mean of 328 ml and a standard deviation of 3 ml.
    <ol>
      <li style="list-style-type: lower-alpha; line-height: 1.1;">
      Calculate the probability that a randomly selected bottle contains less than 330 ml.
      </li>
      <li style="list-style-type: lower-alpha; line-height: 1.1;">
      Calculate the probability that the *average* volume of bottles of beer in a randomly selected crate of 24 bottles contains more than 330 ml.
      </li>
      <li style="list-style-type: lower-alpha; line-height: 1.1;">
      Explain which theorem you used in part b. to calculate the requested probability and whether the conditions to use the theorem were met.
      </li>
    </ol>
  </li>
</ol>
